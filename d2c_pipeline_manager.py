import asyncio
import pandas as pd
import os
from data.serp_discovery import SerpDiscovery
from data.sitemap_parser import SitemapParser
from data.agent_d2c_scanner import AgentD2CScanner

async def run_pipeline():
    print("ğŸš€ [Pipeline] D2C çµäººè‡ªå‹•åŒ–ç³»çµ±å•Ÿå‹•...")
    
    # åˆå§‹åŒ–æ¨¡çµ„
    serp = SerpDiscovery()
    parser = SitemapParser()
    scanner = AgentD2CScanner()
    
    all_products_data = []
    
    # --- Step 1: Target Discovery (ç›®æ¨™é–å®š) ---
    # å»ºè­°ï¼šä½¿ç”¨æ‰‹å‹•ç²¾é¸åå–®ï¼Œå“è³ªé é«˜æ–¼è‡ªå‹•æœå°‹
    MANUAL_TARGETS = [
        "https://www.daikenshop.com",
        "https://www.biomedimei.com",
        "https://www.dietician.com.tw",
        "https://shop.vitabox.com.tw"
    ]
    
    target_domains = set()
    
    if MANUAL_TARGETS:
        print(f"\n--- Phase 1: Manual Target List ({len(MANUAL_TARGETS)} domains) ---")
        target_domains.update(MANUAL_TARGETS)
    else:
        # è‹¥ç„¡æ‰‹å‹•åå–®ï¼Œæ‰å•Ÿç”¨ SERP æœå°‹
        print("\n--- Phase 1: SERP Discovery ---")
        keywords = ["è‘‰é»ƒç´  æ¨è–¦", "é­šæ²¹ æ¨è–¦", "ç›Šç”ŸèŒ å“ç‰Œ"]
        for kw in keywords:
            domains = await serp.search_google(kw, num_results=15)
            target_domains.update(domains)
    
    print(f"ğŸ¯ é–å®š {len(target_domains)} å€‹ç›®æ¨™ç¶²åŸŸ: {list(target_domains)[:5]}...")

    # --- Step 2: Sitemap Parsing (å°èˆª) ---
    print("\n--- Phase 2: Sitemap Parsing ---")
    product_urls_pool = []
    
    for domain in target_domains:
        urls = await parser.parse_sitemap(domain)
        # ç°¡å–®éæ¿¾ï¼šæ¯å€‹ç¶²åŸŸæœ€å¤šå– 10 å€‹ç”¢å“é€£çµæ¸¬è©¦ï¼Œé¿å…æƒæå¤ªä¹…
        product_urls_pool.extend(urls[:10])
    
    print(f"ğŸ”— å…±æå– {len(product_urls_pool)} å€‹ç”¢å“é€£çµå¾…æƒæ")

    # --- Step 3: Agent Scanning (æ¡é›†) ---
    print("\n--- Phase 3: Agent Scanning ---")
    
    # æ‰¹æ¬¡åŸ·è¡Œï¼Œæ¯æ‰¹ 5 å€‹ (é¿å… API Rate Limit)
    batch_size = 5
    for i in range(0, len(product_urls_pool), batch_size):
        batch_urls = product_urls_pool[i : i + batch_size]
        print(f"ğŸ“¦ è™•ç†æ‰¹æ¬¡ {i//batch_size + 1} / {(len(product_urls_pool)//batch_size) + 1}")
        
        results = await scanner.scan_batch(batch_urls)
        all_products_data.extend(results)
        
        # æ‰¹æ¬¡é–“ä¼‘æ¯
        await asyncio.sleep(5)

    # --- Step 4: Save Data (å­˜æª”) ---
    print("\n--- Phase 4: Data Saving ---")
    if all_products_data:
        df = pd.DataFrame(all_products_data)
        
        # ç¢ºä¿æ¬„ä½é †åºç¬¦åˆ Unified Schema
        schema = ["source", "brand", "title", "price", "unit_price", "total_count", "url", "image_url", "product_highlights"]
        # è£œå…¨ç¼ºå¤±æ¬„ä½
        for col in schema:
            if col not in df.columns:
                df[col] = ""
        
        df = df[schema] # é‡æ’
        
        os.makedirs("data", exist_ok=True)
        output_file = "data/d2c_full_database.csv"
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"ğŸ’¾ è³‡æ–™å·²å„²å­˜è‡³: {output_file} (å…± {len(df)} ç­†)")
    else:
        print("âš ï¸ æœ¬æ¬¡ä»»å‹™æœªæ¡é›†åˆ°ä»»ä½•æœ‰æ•ˆè³‡æ–™ã€‚")

if __name__ == "__main__":
    asyncio.run(run_pipeline())