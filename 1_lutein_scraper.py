import requests
import pandas as pd
import time
import re
import random
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# ==========================================
# å·¥å…·å‡½å¼
# ==========================================
def extract_brand(title):
    if not isinstance(title, str): return "æœªæ¨™ç¤º"
    # å˜—è©¦æŠ“å– ã€ã€‘ æˆ– [] è£¡é¢çš„å“ç‰Œ
    match = re.search(r"[ã€\[](.+?)[ã€‘\]]", title)
    if match:
        return match.group(1).strip()
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä¸”æ¨™é¡Œå¤ é•·ï¼Œæš«æ™‚ç”¨å‰å››å€‹å­—ç•¶å“ç‰Œ
    return title[:4] if len(title) > 4 else "æœªæ¨™ç¤º"

def extract_tags(text):
    tags = []
    if not isinstance(text, str): return ""

    # 1. å‹æ…‹ (æ¸¸é›¢å‹å„ªæ–¼é…¯åŒ–å‹)
    if re.search(r"æ¸¸é›¢å‹|Free form", text, re.IGNORECASE):
        tags.append("âœ…æ¸¸é›¢å‹")
    elif re.search(r"é…¯åŒ–å‹|Ester", text, re.IGNORECASE):
        tags.append("âš ï¸é…¯åŒ–å‹")

    # 2. åŸæ–™ (FloraGLO ç‚ºå¤§å» æŒ‡æ¨™)
    if re.search(r"FloraGLO|Kemin", text, re.IGNORECASE):
        tags.append("ğŸ’FloraGLO")
    elif re.search(r"Lutemax", text, re.IGNORECASE):
        tags.append("ğŸ’Lutemax")

    # 3. æ¯”ä¾‹ (10:2 é»ƒé‡‘æ¯”ä¾‹)
    if re.search(r"10[:ï¼š]2|10æ¯”2", text):
        tags.append("âš–ï¸10:2æ¯”ä¾‹")

    # 4. è¤‡æ–¹ (è¦ç´…ç´ ã€èŠ±é’ç´ )
    if re.search(r"è¦ç´…ç´ |è—»ç´…ç´ ", text):
        tags.append("ğŸ¦è¦ç´…ç´ ")
    if re.search(r"èŠ±é’ç´ |å±±æ¡‘å­|é»‘é†‹æ —|æ™ºåˆ©é…’æœ", text):
        tags.append("ğŸ«èŠ±é’ç´ ")

    # æ–°å¢ï¼šé€²éšè¤‡æ–¹ (é‡å°æƒ…å¢ƒ)
    if re.search(r"ç»å°¿é…¸|é­šæ²¹|DHA", text):
        tags.append("ğŸ’§æ°´æ½¤é…æ–¹")
    if re.search(r"è¦ç´…ç´ |é»‘è±†", text):
        tags.append("ğŸ¦èˆ’ç·©å°ˆæ³¨")
    if re.search(r"é¦¬å¥‡è“|å±±æ¡‘å­|èŠ±é’ç´ ", text):
        tags.append("ğŸ«å¤œè¦–å®ˆè­·")

    # æ–°å¢ï¼šåŠ‘å‹åµæ¸¬
    if re.search(r"è† å›Š", text):
        tags.append("ğŸ’Šè† å›Š")
    if re.search(r"é£²|å‡", text):
        tags.append("ğŸ§ƒé£²å“/å‡")

    # 5. æª¢é©—èˆ‡èªè­‰ - æ›´æ–°ç‚ºå…·é«”çš„
    if re.search(r"SNQ", text, re.IGNORECASE):
        tags.append("ğŸ…SNQèªè­‰")
    if re.search(r"SGS", text, re.IGNORECASE):
        tags.append("ğŸ›¡ï¸SGSæª¢é©—")
    if re.search(r"åœ‹å®¶èªè­‰", text, re.IGNORECASE):
        tags.append("ğŸ›¡ï¸ç²èªè­‰")

    # å¦‚æœå®Œå…¨æ²’æœ‰æ¨™ç±¤ï¼Œæ¨™è¨˜ç‚ºä¸€èˆ¬
    if not tags:
        return ""

    return " ".join(tags)

# ==========================================
# 1. PChome çˆ¬èŸ² (ä¿®å¾©å¤§å°å¯«æ•æ„Ÿå•é¡Œ)
# ==========================================
def scrape_pchome_lutein():
    print("ğŸš€ [PChome] é–‹å§‹æŠ“å–...")
    url = "https://ecshweb.pchome.com.tw/search/v3.3/all/results"
    params = {'q': 'è‘‰é»ƒç´ ', 'page': 1, 'sort': 'sale/dc'}
    data_list = []
    
    try:
        for page in range(1, 4): # æŠ“å‰ 3 é 
            params['page'] = page
            res = requests.get(url, params=params)
            if res.status_code == 200:
                products = res.json().get('prods', [])
                print(f"   ğŸ“„ PChome ç¬¬ {page} é æŠ“åˆ° {len(products)} ç­†...")
                
                for p in products:
                    # --- é—œéµä¿®æ­£ï¼šåŒæ™‚å˜—è©¦å¤§å°å¯« key ---
                    name = p.get('Name') or p.get('name') or ""
                    
                    # åƒ¹æ ¼æœ‰æ™‚å€™å« Price, price, æˆ–æ˜¯ originPrice
                    price = p.get('Price') or p.get('price') or p.get('originPrice') or 0
                    
                    pid = p.get('Id') or p.get('id')
                    
                    # åœ–ç‰‡ key ä¹Ÿå¯èƒ½è®Š
                    img_filename = p.get('PicS') or p.get('picS') or p.get('PicB') or p.get('picB')
                    # --------------------------------
                    
                    if img_filename:
                        # è£œä¸Š PChome åœ–ç‰‡ç¶²åŸŸ
                        if img_filename.startswith('http'):
                             image_url = img_filename
                        else:
                             image_url = f"https://cs-a.ecimg.tw{img_filename}"
                    else:
                        image_url = "https://dummyimage.com/200x200/cccccc/ffffff.png&text=No+Image"

                    if not pid: continue
                    
                    data_list.append({
                        "source": "PChome",
                        "brand": extract_brand(name),
                        "title": name,
                        "price": int(price),
                        "url": f"https://24h.pchome.com.tw/prod/{pid}",
                        "image_url": image_url,
                        "tags": extract_tags(name),
                        "sales_volume": 0,  # PChome ä¸æä¾›éŠ·é‡æ•¸æ“šï¼Œé è¨­ç‚º 0
                        "raw_data": name
                    })
            time.sleep(1)
    except Exception as e:
        print(f"âŒ [PChome] éŒ¯èª¤: {e}")
        
    return data_list

# ==========================================
# 2. MOMO çˆ¬èŸ² (éŠ·é‡æ’åºç‰ˆ - å„ªåŒ–æ•ˆç‡)
# ==========================================
def scrape_momo_lutein(limit=100):
    print("ğŸš€ [MOMO] å•Ÿå‹•éš±èº«ç€è¦½å™¨ (éŠ·é‡æ’åº)...")
    data_list = []

    with sync_playwright() as p:
        # 1. å•Ÿå‹•åƒæ•¸ï¼šç§»é™¤è‡ªå‹•åŒ–ç‰¹å¾µ
        browser = p.chromium.launch(
            headless=True,
            args=['--disable-blink-features=AutomationControlled', '--no-sandbox']
        )

        # 2. è¨­ç½® User Agent
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        # 3. æ³¨å…¥ JS éš±è— webdriver å±¬æ€§
        page = context.new_page()
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        try:
            count = 0
            for page_num in range(1, 4):  # åªçˆ¬å‰ 3 é 
                if count >= limit: break
                print(f"ğŸ”— å‰å¾€ MOMO ç¬¬ {page_num} é ...")
                url = f"https://www.momoshop.com.tw/search/searchShop.jsp?keyword=è‘‰é»ƒç´ &searchType=6&curPage={page_num}"
                page.goto(url)
                time.sleep(random.uniform(2, 5))  # åŠ å…¥éš¨æ©Ÿå»¶é²

                # å¢åŠ è¼‰å…¥ç­‰å¾…æ™‚é–“
                try:
                    page.wait_for_selector(".listGoodsData, .goodsUrl", timeout=8000)
                except:
                    print("â³ MOMO è¼‰å…¥è¼ƒæ…¢ï¼Œç¹¼çºŒå˜—è©¦...")

                # æŠ“å–è³‡æ–™ - èª¿æ•´é¸æ“‡å™¨ç¢ºä¿æŠ“åˆ°æ‰€æœ‰å•†å“
                items = page.locator(".listGoodsData").all()
                if not items: items = page.locator(".goodsUrl").all()
                if not items: items = page.locator("li.goodsItemLi").all()
                if not items: items = page.locator(".EachGood").all()
                if not items: items = page.locator("#CategoryContent li").all()

                print(f"ğŸ“¦ MOMO ç¬¬ {page_num} é æ‰¾åˆ° {len(items)} å€‹å•†å“...")

                for item in items:
                    if count >= limit: break
                    try:
                        title = item.locator(".prdName").first.inner_text()
                        print(f"   [é€²åº¦] æ­£åœ¨è§£æç¬¬ {count+1}/{limit} ç­†ï¼š{title[:10]}...", end="\r")

                        price_text = item.locator(".price, .money").first.inner_text()
                        price = int(re.sub(r'[^\d]', '', price_text))

                        link = item.get_attribute("href") or item.locator("a").first.get_attribute("href")
                        if link and not link.startswith("http"): link = "https://www.momoshop.com.tw" + link

                        # é€²å…¥å…§é æŠ“å–è©³ç´°è³‡è¨Š - ä½¿ç”¨æ–°åˆ†é é¿å…å½±éŸ¿åˆ—è¡¨é 
                        inner_text = ""
                        if link:
                            new_page = None
                            try:
                                new_page = context.new_page()
                                new_page.goto(link, timeout=10000)
                                time.sleep(random.uniform(2, 5))  # åŠ å…¥éš¨æ©Ÿå»¶é²
                                try:
                                    inner_text = new_page.locator('.spec, .description, #spec').first.inner_text()
                                except:
                                    inner_text = ""
                            except Exception as e:
                                print(f"âŒ å…§é æŠ“å–å¤±æ•— ({link}): {e}")
                                inner_text = ""
                            finally:
                                if new_page:
                                    try:
                                        new_page.close()
                                    except:
                                        pass

                        # åœ–ç‰‡æŠ“å–
                        image_url = None
                        imgs = item.locator("img").all()
                        for img in imgs:
                            src = img.get_attribute("data-original") or img.get_attribute("src")
                            # éæ¿¾ç„¡æ•ˆåœ–ç‰‡
                            if src and "ecm" not in src and "icon" not in src:
                                if "goodsimg" in src or "i1.momoshop" in src:
                                    image_url = src
                                    break
                                if not image_url and "dummy" not in src and "data:image" not in src:
                                    image_url = src

                        if not image_url: image_url = "https://dummyimage.com/200x200/cccccc/ffffff.png&text=MOMO+No+Img"

                        # æŠ“å–éŠ·é‡ - å¦‚æœæŠ“ä¸åˆ°é è¨­ç‚º 0
                        sales_volume = 0
                        try:
                            slogan_text = item.locator(".money .slogan").first.inner_text()
                            match = re.search(r'ç¸½éŠ·é‡\D*(\d+(?:,\d+)*)', slogan_text)  # æ”¾å¯¬ Regex
                            if match:
                                sales_volume = int(match.group(1).replace(',', ''))
                        except:
                            pass

                        # åˆä½µ title å’Œå…§é æ–‡å­—ç”¨æ–¼ extract_tags
                        combined_text = title + " " + inner_text
                        tags = extract_tags(combined_text)

                        data_list.append({
                            "source": "MOMO",
                            "brand": extract_brand(title),
                            "title": title,
                            "price": price,
                            "url": link,
                            "image_url": image_url,
                            "tags": tags,
                            "sales_volume": sales_volume,
                            "raw_data": title
                        })
                        count += 1
                    except Exception as e:
                        print(f"âŒ å•†å“æŠ“å–å¤±æ•—: {e}")
                        # å³ä½¿å¤±æ•—ï¼Œä¹Ÿå˜—è©¦è¨˜éŒ„åŸºæœ¬è³‡æ–™ (æ¨™é¡Œã€åƒ¹æ ¼)
                        try:
                            basic_title = item.locator(".prdName").first.inner_text()
                            basic_price_text = item.locator(".price, .money").first.inner_text()
                            basic_price = int(re.sub(r'[^\d]', '', basic_price_text))
                            basic_link = item.get_attribute("href") or item.locator("a").first.get_attribute("href")
                            if basic_link and not basic_link.startswith("http"): basic_link = "https://www.momoshop.com.tw" + basic_link

                            data_list.append({
                                "source": "MOMO",
                                "brand": extract_brand(basic_title),
                                "title": basic_title,
                                "price": basic_price,
                                "url": basic_link,
                                "image_url": "https://dummyimage.com/200x200/cccccc/ffffff.png&text=MOMO+Basic",
                                "tags": "",
                                "sales_volume": 0,
                                "raw_data": basic_title
                            })
                            count += 1
                        except:
                            continue

                time.sleep(1)

        except Exception as e:
            print(f"âŒ [MOMO] éŒ¯èª¤: {e}")
        finally:
            browser.close()

    return data_list

# ==========================================
# ä¸»ç¨‹å¼
# ==========================================
if __name__ == "__main__":
    # 1. åŸ·è¡Œ PChome
    df_p = pd.DataFrame(scrape_pchome_lutein())
    
    # 2. åŸ·è¡Œ MOMO (é™åˆ¶å‰ 30 ç­†å•†å“)
    df_m = pd.DataFrame(scrape_momo_lutein(30))
    
    # 3. åˆä½µèˆ‡å­˜æª”
    all_df = pd.concat([df_p, df_m], ignore_index=True)
    
    if not all_df.empty:
        all_df.to_csv("lutein_market_data.csv", index=False, encoding="utf-8-sig")
        print("\nâœ… è³‡æ–™åˆä½µå®Œæˆï¼")
        print(f"   PChome: {len(df_p)} ç­†")
        print(f"   MOMO:   {len(df_m)} ç­†")
        
        # ç°¡å–®æª¢æŸ¥å‰å¹¾ç­† PChome æ˜¯å¦æœ‰æŠ“åˆ°æ¨™é¡Œ
        print("\nğŸ” è³‡æ–™æŠ½æ¨£æª¢æŸ¥ (PChome):")
        print(df_p[['title', 'price']].head(3))
    else:
        print("âš ï¸ å®Œå…¨æ²’æŠ“åˆ°è³‡æ–™ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–ç¨‹å¼ç¢¼ã€‚")
