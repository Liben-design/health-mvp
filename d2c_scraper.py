import asyncio
import pandas as pd
import time
import re
import random
import os
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from playwright_stealth import stealth_sync
from bs4 import BeautifulSoup

# ==========================================
# å…±äº«å·¥å…·å‡½å¼ (å¾ general_scraper.py ç§»è½‰)
# ==========================================
BRAND_WHITELIST = [
    "å¤§ç ”ç”Ÿé†«", "ç‡Ÿé¤Šå¸«è¼•é£Ÿ", "Swisse", "Nature's Way", "Blackmores", "GNC",
    "Kemin", "FloraGLO", "Lutemax", "DSM", "BASF", "NOW Foods", "Doctor's Best"
]

def extract_brand(title):
    if not isinstance(title, str): return "æœªæ¨™ç¤º"
    for brand in BRAND_WHITELIST:
        if brand.lower() in title.lower():
            return brand
    match = re.search(r"[ã€\[](.+?)[ã€‘\]]", title)
    if match: return match.group(1).strip()
    return title[:4] if len(title) > 4 else "æœªæ¨™ç¤º"

def calculate_unit_price(title, price):
    if not isinstance(title, str): return None, 0
    unit_count, bundle_size = None, 1
    match = re.search(r'(\d+)\s*[ç²’é¡†éŒ ]', title)
    if match: unit_count = int(match.group(1))
    match = re.search(r'(\d+)\s*[å…¥ä»¶ç›’ç½åŒ…]çµ„?', title)
    if match: bundle_size = int(match.group(1))
    else:
        match = re.search(r'[xX*]\s*(\d+)', title)
        if match: bundle_size = int(match.group(1))
    if unit_count:
        total_count = unit_count * bundle_size
        return total_count, round(price / total_count, 2)
    return None, 0

def extract_tags(text):
    tags = []
    if not isinstance(text, str): return ""
    # ç°¡åŒ–ç‰ˆæ¨™ç±¤æå–ï¼Œå¯æ ¹æ“šD2Cçš„è©³ç´°æè¿°é€²è¡Œæ“´å……
    if re.search(r"æ¸¸é›¢å‹|Free form", text, re.IGNORECASE): tags.append("âœ…æ¸¸é›¢å‹")
    if re.search(r"FloraGLO|Kemin", text, re.IGNORECASE): tags.append("ğŸ’FloraGLO")
    if re.search(r"10[:ï¼š]2", text): tags.append("âš–ï¸10:2æ¯”ä¾‹")
    if re.search(r"è¦ç´…ç´ |è—»ç´…ç´ ", text): tags.append("ğŸ¦è¦ç´…ç´ ")
    if re.search(r"èŠ±é’ç´ |å±±æ¡‘å­", text): tags.append("ğŸ«èŠ±é’ç´ ")
    if re.search(r"ç»å°¿é…¸|é­šæ²¹|DHA", text): tags.append("ğŸ’§æ°´æ½¤é…æ–¹")
    if re.search(r"SNQ", text, re.IGNORECASE): tags.append("ğŸ…SNQèªè­‰")
    if re.search(r"SGS", text, re.IGNORECASE): tags.append("ğŸ›¡ï¸SGSæª¢é©—")
    return " ".join(tags) if tags else ""

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]

# ==========================================
# D2C å“ç‰Œè¨­å®šæª”
# ==========================================
DAIKEN_CONFIG = {
    "brand_name": "å¤§ç ”ç”Ÿé†«",
    "product_list_url": "https://www.daikenshop.com/allgoods.php",
    # "direct_links": [
    #     "https://www.daikenshop.com/product.php?code=4710255450081" # è¦–æ˜“é©è‘‰é»ƒç´ 
    # ],
    "selectors": {
        # åˆ—è¡¨é é¸æ“‡å™¨ (å‚™ç”¨)
        "list_item": ".product-wrap",
        "list_title": "h3.product-name",
        "product_url": ".product-image a",
        "product_img": ".product-image img",
        "product_price": ".product-price",
        # è©³æƒ…é é¸æ“‡å™¨
        "details": {
            "title": "h1.product-name",
            "description": ".product-description",
            "ingredients": ".product-description" # æŠ“å–æ•´å€‹æè¿°å€å¡Šè®“tagæå–
        }
    }
}

# ==========================================
# æ ¸å¿ƒçˆ¬èŸ²å‡½å¼
# ==========================================
def scrape_d2c_site(config, keyword_filter, max_retries=2):
    print(f"ğŸš€ [D2C Scraper] å•Ÿå‹•ç€è¦½å™¨ï¼Œç›®æ¨™å“ç‰Œï¼š{config['brand_name']}")
    data_list = []
    product_links = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent=random.choice(USER_AGENTS))
        
        try:
            # --- ç­–ç•¥é¸æ“‡ï¼šå„ªå…ˆä½¿ç”¨ç›´æ¥é€£çµï¼Œå¦å‰‡å¾åˆ—è¡¨é ç™¼ç¾ ---
            if "direct_links" in config and config["direct_links"]:
                print("ğŸ¯ ä½¿ç”¨ç›´æ¥é€£çµæ¨¡å¼...")
                product_links = config["direct_links"]
            else:
                print("ğŸ§­ ä½¿ç”¨åˆ—è¡¨é ç™¼ç¾æ¨¡å¼...")
                page = context.new_page()
                stealth_sync(page)

                # 1. å‰å¾€ç”¢å“åˆ—è¡¨é 
                print(f"ğŸ”— å‰å¾€åˆ—è¡¨é : {config['product_list_url']}")
                page.goto(config['product_list_url'], wait_until="domcontentloaded", timeout=60000)
                try:
                    # é»æ“Š Cookie åŒæ„æŒ‰éˆ•
                    print("... æ­£åœ¨å°‹æ‰¾ä¸¦é»æ“Š Cookie åŒæ„æŒ‰éˆ• ...")
                    agree_button = page.locator('text="åŒæ„"').first
                    agree_button.click(timeout=5000)
                    print("âœ… Cookie åŒæ„æŒ‰éˆ•å·²é»æ“Šã€‚")
                except Exception as e:
                    print("â„¹ï¸ æœªæ‰¾åˆ° Cookie åŒæ„æŒ‰éˆ•ï¼Œæˆ–é»æ“Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œç¹¼çºŒåŸ·è¡Œ...")
                
                print("â³ ç­‰å¾…ç”¢å“åˆ—è¡¨å‡ºç¾...")
                page.wait_for_selector(config["selectors"]["list_item"], timeout=20000)
                print("âœ… ç”¢å“åˆ—è¡¨å·²è¼‰å…¥ã€‚")

                # 2. æ¨¡æ“¬æ»¾å‹•ï¼Œè¼‰å…¥æ‰€æœ‰å•†å“
                print("ğŸ”„ æ¨¡æ“¬æ»¾å‹•ä»¥è¼‰å…¥æ‰€æœ‰å•†å“...")
                for _ in range(5): # æ»¾å‹•5æ¬¡ä»¥ç›¡å¯èƒ½è¼‰å…¥
                    page.mouse.wheel(0, 15000)
                    time.sleep(random.uniform(2, 4))

                # 3. æŠ“å–æ‰€æœ‰å•†å“é€£çµ
                all_items = page.locator(config["selectors"]["list_item"]).all()
                print(f"ğŸ•µï¸â€â™‚ï¸ æ‰¾åˆ° {len(all_items)} å€‹ç”¢å“é …ç›®ï¼Œé–‹å§‹éæ¿¾...")
                for item in all_items:
                    try:
                        # éæ¿¾å‡ºåŒ…å«é—œéµå­—çš„å•†å“
                        title_text = item.locator(config["selectors"]["list_title"]).inner_text()
                        print(f"   - æ­£åœ¨æª¢æŸ¥: {title_text.strip()}") # é™¤éŒ¯ï¼šå°å‡ºæ‰€æœ‰æŠ“åˆ°çš„æ¨™é¡Œ
                        if keyword_filter.lower() in title_text.lower():
                            link = item.locator(config["selectors"]["product_url"]).get_attribute("href")
                            if link and not link.startswith("http"):
                                base_url = config['product_list_url'].split('/allgoods.php')[0]
                                link = base_url + "/" + link.lstrip("/")
                            product_links.append(link)
                    except Exception as e:
                        print(f"âš ï¸ åˆ—è¡¨é …ç›®è§£æéŒ¯èª¤: {e}")
                
                page.close()

            print(f"âœ… å…±éœ€æŠ“å– {len(product_links)} å€‹å•†å“é€£çµã€‚")

            # 4. é€ä¸€é€²å…¥è©³æƒ…é æŠ“å–
            for i, link in enumerate(product_links):
                if not link: continue
                print(f"   [é€²åº¦ {i+1}/{len(product_links)}] æ­£åœ¨è§£æ: {link}")
                
                # --- æ§åˆ¶è«–ï¼šéŒ¯èª¤é‡è©¦æ©Ÿåˆ¶ ---
                for attempt in range(max_retries):
                    detail_page = None
                    try:
                        # åœ¨æ¯æ¬¡å¾ªç’°ä¸­å‰µå»ºæ–°é é¢
                        detail_page = context.new_page()
                        stealth_sync(detail_page)
                        
                        detail_page.goto(link, wait_until="domcontentloaded", timeout=30000)
                        try:
                            # é»æ“Š Cookie åŒæ„æŒ‰éˆ•
                            print("... æ­£åœ¨å°‹æ‰¾ä¸¦é»æ“Š Cookie åŒæ„æŒ‰éˆ• ...")
                            agree_button = detail_page.locator('text="åŒæ„"').first
                            agree_button.click(timeout=5000)
                            print("âœ… Cookie åŒæ„æŒ‰éˆ•å·²é»æ“Šã€‚")
                        except Exception as e:
                            print("â„¹ï¸ æœªæ‰¾åˆ° Cookie åŒæ„æŒ‰éˆ•ï¼Œæˆ–é»æ“Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œç¹¼çºŒåŸ·è¡Œ...")
                        
                        time.sleep(random.uniform(2, 3))

                        # --- æŠ“å–æ ¸å¿ƒæ•¸æ“š ---
                        detail_page.wait_for_selector(config["selectors"]["details"]["title"], state='visible', timeout=60000)
                        title = detail_page.locator(config["selectors"]["details"]["title"]).inner_text()
                        price_text = detail_page.locator(config["selectors"]["product_price"]).first.inner_text()
                        price = int(re.sub(r'[^\d]', '', price_text))

                        # --- æ“´å……æ•¸æ“šå­—æ®µ ---
                        description = detail_page.locator(config["selectors"]["details"]["description"]).first.inner_text()
                        ingredients = detail_page.locator(config["selectors"]["details"]["ingredients"]).first.inner_text()
                        
                        # åœ¨è©³æƒ…é é‡æ–°æŠ“å–åœ–ç‰‡ï¼Œç¢ºä¿æ˜¯æœ€é«˜ç•«è³ª
                        img_element = detail_page.locator(config["selectors"]["product_img"]).first
                        image_url = img_element.get_attribute("src") or img_element.get_attribute("data-src")
                        if image_url and image_url.startswith('//'):
                            image_url = 'https:' + image_url

                        # --- æ•´åˆèˆ‡æ¸…æ´— ---
                        title = title.replace(",", " ").replace("\n", " ")
                        full_text = f"{title} {description} {ingredients}"
                        
                        total_count, unit_price = calculate_unit_price(title, price)
                        tags = extract_tags(full_text)

                        data_list.append({
                            "source": config["brand_name"],
                            "brand": config["brand_name"],
                            "title": title,
                            "price": price,
                            "url": link,
                            "image_url": image_url,
                            "tags": tags,
                            "sales_volume": 0, # D2C ç„¡æ³•å¾—çŸ¥éŠ·é‡
                            "raw_data": f"{title} {description}",
                            "total_count": total_count,
                            "unit_price": unit_price
                        })
                        
                        break # æˆåŠŸï¼Œè·³å‡ºé‡è©¦å¾ªç’°

                    except Exception as e:
                        print(f"      âŒ ç¬¬ {attempt+1} æ¬¡æŠ“å–å¤±æ•—: {e}")
                        if attempt == max_retries - 1:
                            print(f"      â€¼ï¸ ç„¡æ³•æŠ“å–è©²é é¢ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œè·³éã€‚")
                            if detail_page:
                                detail_page.screenshot(path=f"debug_screenshot.png")
                        else:
                            time.sleep(random.uniform(3, 5)) # ç­‰å¾…å¾Œé‡è©¦
                    finally:
                        if detail_page:
                            try:
                                detail_page.close()
                            except:
                                pass # é é¢å¯èƒ½å·²å› éŒ¯èª¤è€Œé—œé–‰

        except Exception as e:
            print(f"âŒ [D2C Scraper] ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        finally:
            browser.close()
            print("âœ… ç€è¦½å™¨å·²é—œé–‰ã€‚")

    return data_list

# ==========================================
# ä¸»ç¨‹å¼åŸ·è¡Œå€
# ==========================================
if __name__ == "__main__":
    # --- ä»»å‹™è¨­å®š ---
    TARGET_KEYWORD = "è¦–æ˜“é©è‘‰é»ƒç´ "
    
    # 1. åŸ·è¡Œ D2C çˆ¬èŸ²
    d2c_data = scrape_d2c_site(DAIKEN_CONFIG, keyword_filter=TARGET_KEYWORD)
    df_d2c = pd.DataFrame(d2c_data)

    # 2. æª¢æŸ¥èˆ‡å­˜æª”
    if not df_d2c.empty:
        # ç¢ºä¿å­˜æª”è³‡æ–™å¤¾å­˜åœ¨
        os.makedirs("data", exist_ok=True)
        
        # --- è¼¸å‡º CSVï¼Œèˆ‡ç¾æœ‰æ ¼å¼å…¼å®¹ ---
        filename = f"data/D2C_{TARGET_KEYWORD}_data.csv"
        df_d2c.to_csv(filename, index=False, encoding="utf-8-sig")
        
        print("\n\n" + "="*50)
        print("ğŸ‰ D2C çˆ¬èŸ²ä»»å‹™å®Œæˆï¼")
        print(f"ğŸ’¾ è³‡æ–™å·²å­˜æª”è‡³: {filename}")
        print(f"ç¸½å…±æŠ“å–åˆ° {len(df_d2c)} ç­† '{TARGET_KEYWORD}' ç›¸é—œå•†å“ã€‚")
        print("="*50)

        # --- æä¾›æ¸¬è©¦æŠ“å–çš„çµæœç¯„ä¾‹ ---
        print("\nğŸ“œ è³‡æ–™ç¯„ä¾‹é è¦½ï¼š")
        print(df_d2c[['brand', 'title', 'price', 'tags']].head())
    else:
        print("\nâš ï¸ æœ¬æ¬¡ D2C çˆ¬èŸ²æœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ã€‚")

    print("\n\n--- å¿«é€Ÿæ“´å……æŒ‡å— ---")
    print("å¦‚ä½•æ”¯æ´ä¸‹ä¸€å€‹ D2C å“ç‰Œï¼Ÿ")
    print("1. åœ¨ d2c_scraper.py ä¸­ï¼Œä»¿ç…§ DAIKEN_CONFIG å»ºç«‹ä¸€å€‹æ–°çš„è¨­å®šæª”ï¼Œä¾‹å¦‚ LITE_CONFIGã€‚")
    print("2. å¡«å¯«æ–°å“ç‰Œçš„ `brand_name`, `product_list_url`ã€‚")
    print("3. æ‰‹å‹•è§€å¯Ÿæ–°å“ç‰Œç¶²ç«™çš„ HTML çµæ§‹ï¼Œæ›´æ–° `selectors` å­—å…¸ä¸­çš„ CSS é¸æ“‡å™¨ã€‚")
    print("4. åœ¨ä¸»ç¨‹å¼å€å¡Šï¼Œå‘¼å« `scrape_d2c_site(LITE_CONFIG, ...)` å³å¯é–‹å§‹æŠ“å–ã€‚")
    print("å·¥åŒ åŸå‰‡çš„æ ¸å¿ƒåœ¨æ–¼æ¨¡çµ„åŒ–ï¼Œè®“æ“´å……è®Šå¾—ç°¡å–®ï¼")
