import asyncio
import pandas as pd
import time
import re
import random
import os
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup

# ==========================================
# å…±äº«å·¥å…·å‡½å¼ (å¾ general_scraper.py ç§»è½‰)
# ==========================================
BRAND_WHITELIST = [
    "å¤§ç ”ç”Ÿé†«", "ç‡Ÿé¤Šå¸«è¼•é£Ÿ", "Swisse", "Nature's Way", "Blackmores", "GNC",
    "Kemin", "FloraGLO", "Lutemax", "DSM", "BASF", "NOW Foods", "Doctor's Best"
]

def extract_brand(title):
    if not isinstance(title, str): return "æœªæ¨™ç¤º"
    for brand in BRAND_WHITELIST:
        if brand.lower() in title.lower():
            return brand
    match = re.search(r"[ã€\[](.+?)[ã€‘\]]", title)
    if match: return match.group(1).strip()
    return title[:4] if len(title) > 4 else "æœªæ¨™ç¤º"

def calculate_unit_price(title, price):
    if not isinstance(title, str): return None, 0
    unit_count, bundle_size = None, 1
    match = re.search(r'(\d+)\s*[ç²’é¡†éŒ ]', title)
    if match: unit_count = int(match.group(1))
    match = re.search(r'(\d+)\s*[å…¥ä»¶ç›’ç½åŒ…]çµ„?', title)
    if match: bundle_size = int(match.group(1))
    else:
        match = re.search(r'[xX*]\s*(\d+)', title)
        if match: bundle_size = int(match.group(1))
    if unit_count:
        total_count = unit_count * bundle_size
        return total_count, round(price / total_count, 2)
    return None, 0

def extract_tags(text):
    tags = []
    if not isinstance(text, str): return ""
    # ç°¡åŒ–ç‰ˆæ¨™ç±¤æå–ï¼Œå¯æ ¹æ“šD2Cçš„è©³ç´°æè¿°é€²è¡Œæ“´å……
    if re.search(r"æ¸¸é›¢å‹|Free form", text, re.IGNORECASE): tags.append("âœ…æ¸¸é›¢å‹")
    if re.search(r"FloraGLO|Kemin", text, re.IGNORECASE): tags.append("ğŸ’FloraGLO")
    if re.search(r"10[:ï¼š]2", text): tags.append("âš–ï¸10:2æ¯”ä¾‹")
    if re.search(r"è¦ç´…ç´ |è—»ç´…ç´ ", text): tags.append("ğŸ¦è¦ç´…ç´ ")
    if re.search(r"èŠ±é’ç´ |å±±æ¡‘å­", text): tags.append("ğŸ«èŠ±é’ç´ ")
    if re.search(r"ç»å°¿é…¸|é­šæ²¹|DHA", text): tags.append("ğŸ’§æ°´æ½¤é…æ–¹")
    if re.search(r"SNQ", text, re.IGNORECASE): tags.append("ğŸ…SNQèªè­‰")
    if re.search(r"SGS", text, re.IGNORECASE): tags.append("ğŸ›¡ï¸SGSæª¢é©—")
    return " ".join(tags) if tags else ""

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]

# ==========================================
# D2C å“ç‰Œè¨­å®šæª”
# ==========================================
DAIKEN_CONFIG = {
    "brand_name": "å¤§ç ”ç”Ÿé†«",
    # æŠ“å–æ‰€æœ‰ä¿å¥é£Ÿå“ï¼Œå¾ŒçºŒå†ç”¨é—œéµå­—éæ¿¾
    "product_list_url": "https://www.daiken.com.tw/collections/all-products",
    "selectors": {
        # åˆ—è¡¨é é¸æ“‡å™¨
        "list_item": ".product-item",
        "product_url": "a.product-item-meta-title",
        "product_img": ".product-item-img-wrapper img",
        "product_price": ".price-item--regular",
        # è©³æƒ…é é¸æ“‡å™¨
        "details": {
            "title": ".product-meta__title",
            "description": ".product-description-container",
            "ingredients": ".product-info-item.ingredients"
        }
    }
}

# ==========================================
# æ ¸å¿ƒçˆ¬èŸ²å‡½å¼
# ==========================================
def scrape_d2c_site(config, keyword_filter, max_retries=2):
    print(f"ğŸš€ [D2C Scraper] å•Ÿå‹•ç€è¦½å™¨ï¼Œç›®æ¨™å“ç‰Œï¼š{config['brand_name']}")
    data_list = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=['--disable-blink-features=AutomationControlled'])
        context = browser.new_context(user_agent=random.choice(USER_AGENTS))
        page = context.new_page()
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        try:
            # 1. å‰å¾€ç”¢å“åˆ—è¡¨é 
            print(f"ğŸ”— å‰å¾€åˆ—è¡¨é : {config['product_list_url']}")
            page.goto(config['product_list_url'], wait_until="domcontentloaded", timeout=60000)
            
            # 2. æ¨¡æ“¬æ»¾å‹•ï¼Œè¼‰å…¥æ‰€æœ‰å•†å“
            print("ğŸ”„ æ¨¡æ“¬æ»¾å‹•ä»¥è¼‰å…¥æ‰€æœ‰å•†å“...")
            for _ in range(5): # æ»¾å‹•5æ¬¡ä»¥ç›¡å¯èƒ½è¼‰å…¥
                page.mouse.wheel(0, 15000)
                time.sleep(random.uniform(2, 4))

            # 3. æŠ“å–æ‰€æœ‰å•†å“é€£çµ
            all_items = page.locator(config["selectors"]["list_item"]).all()
            product_links = []
            for item in all_items:
                try:
                    # éæ¿¾å‡ºåŒ…å«é—œéµå­—çš„å•†å“
                    title_text = item.locator('a.product-item-meta-title').inner_text()
                    if keyword_filter.lower() in title_text.lower():
                        link = item.locator(config["selectors"]["product_url"]).get_attribute("href")
                        if link and not link.startswith("http"):
                            base_url = config['product_list_url'].split('/collections')[0]
                            link = base_url + link
                        product_links.append(link)
                except Exception as e:
                    print(f"âš ï¸ åˆ—è¡¨é …ç›®è§£æéŒ¯èª¤: {e}")
            
            print(f"âœ… æ‰¾åˆ° {len(product_links)} å€‹ç¬¦åˆ '{keyword_filter}' çš„å•†å“é€£çµã€‚")

            # 4. é€ä¸€é€²å…¥è©³æƒ…é æŠ“å–
            for i, link in enumerate(product_links):
                if not link: continue
                print(f"   [é€²åº¦ {i+1}/{len(product_links)}] æ­£åœ¨è§£æ: {link}")
                
                # --- æ§åˆ¶è«–ï¼šéŒ¯èª¤é‡è©¦æ©Ÿåˆ¶ ---
                for attempt in range(max_retries):
                    detail_page = None
                    try:
                        detail_page = context.new_page()
                        detail_page.goto(link, wait_until="domcontentloaded", timeout=30000)
                        time.sleep(random.uniform(2, 3))

                        # --- æŠ“å–æ ¸å¿ƒæ•¸æ“š ---
                        title = detail_page.locator(config["selectors"]["details"]["title"]).inner_text()
                        price_text = detail_page.locator(config["selectors"]["product_price"]).first.inner_text()
                        price = int(re.sub(r'[^\d]', '', price_text))

                        # --- æ“´å……æ•¸æ“šå­—æ®µ ---
                        description = detail_page.locator(config["selectors"]["details"]["description"]).first.inner_text()
                        ingredients = detail_page.locator(config["selectors"]["details"]["ingredients"]).first.inner_text()
                        
                        image_url = detail_page.locator(config["selectors"]["product_img"]).first.get_attribute("src")
                        if image_url and image_url.startswith('//'):
                            image_url = 'https:' + image_url

                        # --- æ•´åˆèˆ‡æ¸…æ´— ---
                        title = title.replace(",", " ").replace("\n", " ")
                        full_text = f"{title} {description} {ingredients}"
                        
                        total_count, unit_price = calculate_unit_price(title, price)
                        tags = extract_tags(full_text)

                        data_list.append({
                            "source": config["brand_name"],
                            "brand": config["brand_name"],
                            "title": title,
                            "price": price,
                            "url": link,
                            "image_url": image_url,
                            "tags": tags,
                            "sales_volume": 0, # D2C ç„¡æ³•å¾—çŸ¥éŠ·é‡
                            "raw_data": f"{title} {description}",
                            "total_count": total_count,
                            "unit_price": unit_price
                        })
                        
                        detail_page.close()
                        break # æˆåŠŸï¼Œè·³å‡ºé‡è©¦å¾ªç’°

                    except Exception as e:
                        print(f"      âŒ ç¬¬ {attempt+1} æ¬¡æŠ“å–å¤±æ•—: {e}")
                        if detail_page: detail_page.close()
                        if attempt == max_retries - 1:
                            print(f"      â€¼ï¸ ç„¡æ³•æŠ“å–è©²é é¢ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œè·³éã€‚")
                        else:
                            time.sleep(random.uniform(3, 5)) # ç­‰å¾…å¾Œé‡è©¦

        except Exception as e:
            print(f"âŒ [D2C Scraper] ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        finally:
            browser.close()
            print("âœ… ç€è¦½å™¨å·²é—œé–‰ã€‚")

    return data_list

# ==========================================
# ä¸»ç¨‹å¼åŸ·è¡Œå€
# ==========================================
if __name__ == "__main__":
    # --- ä»»å‹™è¨­å®š ---
    TARGET_KEYWORD = "è‘‰é»ƒç´ "
    
    # 1. åŸ·è¡Œ D2C çˆ¬èŸ²
    d2c_data = scrape_d2c_site(DAIKEN_CONFIG, keyword_filter=TARGET_KEYWORD)
    df_d2c = pd.DataFrame(d2c_data)

    # 2. æª¢æŸ¥èˆ‡å­˜æª”
    if not df_d2c.empty:
        # ç¢ºä¿å­˜æª”è³‡æ–™å¤¾å­˜åœ¨
        os.makedirs("data", exist_ok=True)
        
        # --- è¼¸å‡º CSVï¼Œèˆ‡ç¾æœ‰æ ¼å¼å…¼å®¹ ---
        filename = f"data/D2C_{TARGET_KEYWORD}_data.csv"
        df_d2c.to_csv(filename, index=False, encoding="utf-8-sig")
        
        print("\n\n" + "="*50)
        print("ğŸ‰ D2C çˆ¬èŸ²ä»»å‹™å®Œæˆï¼")
        print(f"ğŸ’¾ è³‡æ–™å·²å­˜æª”è‡³: {filename}")
        print(f"ç¸½å…±æŠ“å–åˆ° {len(df_d2c)} ç­† '{TARGET_KEYWORD}' ç›¸é—œå•†å“ã€‚")
        print("="*50)

        # --- æä¾›æ¸¬è©¦æŠ“å–çš„çµæœç¯„ä¾‹ ---
        print("\nğŸ“œ è³‡æ–™ç¯„ä¾‹é è¦½ï¼š")
        print(df_d2c[['brand', 'title', 'price', 'tags']].head())
    else:
        print("\nâš ï¸ æœ¬æ¬¡ D2C çˆ¬èŸ²æœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ã€‚")

    print("\n\n--- å¿«é€Ÿæ“´å……æŒ‡å— ---")
    print("å¦‚ä½•æ”¯æ´ä¸‹ä¸€å€‹ D2C å“ç‰Œï¼Ÿ")
    print("1. åœ¨ d2c_scraper.py ä¸­ï¼Œä»¿ç…§ DAIKEN_CONFIG å»ºç«‹ä¸€å€‹æ–°çš„è¨­å®šæª”ï¼Œä¾‹å¦‚ LITE_CONFIGã€‚")
    print("2. å¡«å¯«æ–°å“ç‰Œçš„ `brand_name`, `product_list_url`ã€‚")
    print("3. æ‰‹å‹•è§€å¯Ÿæ–°å“ç‰Œç¶²ç«™çš„ HTML çµæ§‹ï¼Œæ›´æ–° `selectors` å­—å…¸ä¸­çš„ CSS é¸æ“‡å™¨ã€‚")
    print("4. åœ¨ä¸»ç¨‹å¼å€å¡Šï¼Œå‘¼å« `scrape_d2c_site(LITE_CONFIG, ...)` å³å¯é–‹å§‹æŠ“å–ã€‚")
    print("å·¥åŒ åŸå‰‡çš„æ ¸å¿ƒåœ¨æ–¼æ¨¡çµ„åŒ–ï¼Œè®“æ“´å……è®Šå¾—ç°¡å–®ï¼")
