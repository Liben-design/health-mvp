import asyncio
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
from .base_scraper import BaseScraper

class DaikenScraper(BaseScraper):
    def __init__(self):
        # åˆå§‹åŒ–çˆ¶é¡åˆ¥ï¼ŒæŒ‡å®šè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        super().__init__("data/d2c_daiken_all_products.csv")
        self.list_url = "https://www.daikenshop.com/allgoods.php"
        self.base_url = "https://www.daikenshop.com"

    async def run(self):
        print(f"ğŸš€ [DaikenScraper] å•Ÿå‹•çˆ¬èŸ²...")
        
        async with async_playwright() as p:
            # å•Ÿå‹•ç€è¦½å™¨ (ä½¿ç”¨çˆ¶é¡åˆ¥å®šç¾©çš„ User-Agent)
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent=random.choice(self.user_agents),
                viewport={'width': 1920, 'height': 1080}
            )
            page = await context.new_page()
            await stealth_async(page)

            # --- éšæ®µ 1: æŠ“å–ç”¢å“åˆ—è¡¨ ---
            print(f"ğŸ”— å‰å¾€åˆ—è¡¨é : {self.list_url}")
            await page.goto(self.list_url, wait_until='networkidle')
            
            # è™•ç† Cookie åŒæ„æŒ‰éˆ•
            try:
                if await page.locator('text="åŒæ„"').count() > 0:
                    await page.locator('text="åŒæ„"').first.click()
            except: pass

            # æ»¾å‹•é é¢ç¢ºä¿è¼‰å…¥
            for _ in range(3):
                await page.evaluate('window.scrollBy(0, window.innerHeight)')
                await asyncio.sleep(1)

            # è§£ææ‰€æœ‰ç”¢å“é€£çµ
            content = await page.content()
            soup = BeautifulSoup(content, 'html.parser')
            links = []
            for a in soup.find_all('a', href=True):
                if 'product.php?code=' in a['href']:
                    links.append(urljoin(self.base_url, a['href']))
            
            links = list(set(links))
            print(f"ğŸ“Š ç™¼ç¾ {len(links)} å€‹ç”¢å“é€£çµ")

            # --- éšæ®µ 2: éæ­·è©³æƒ…é  ---
            for i, link in enumerate(links):
                print(f"   [{i+1}/{len(links)}] è™•ç†: {link}")
                try:
                    await page.goto(link, wait_until='networkidle', timeout=60000)
                    await self.random_sleep(2, 4)
                    
                    # è§£æè©³æƒ…
                    content = await page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # æå–è³‡æ–™
                    h1 = soup.find('h1')
                    title = h1.get_text(strip=True) if h1 else "Unknown"
                    
                    # åƒ¹æ ¼è™•ç†
                    price_tag = soup.find(string=re.compile("å„ªæƒ åƒ¹"))
                    price_text = price_tag.parent.get_text() if price_tag else "0"
                    price = int(re.sub(r'[^\d]', '', price_text) or 0)
                    
                    # åœ–ç‰‡ (å„ªå…ˆä½¿ç”¨ og:image)
                    og_img = soup.find("meta", property="og:image")
                    image_url = og_img["content"] if og_img else ""
                    
                    # æè¿°èˆ‡è¦æ ¼ (ç”¨æ–¼è¨ˆç®—å–®åƒ¹èˆ‡æå–æ¨™ç±¤)
                    desc_text = ""
                    for selector in [".product-description", ".detail_content"]:
                        for el in soup.select(selector):
                            desc_text += el.get_text(" ", strip=True)
                    
                    # ä½¿ç”¨çˆ¶é¡åˆ¥çš„å·¥å…·å‡½å¼
                    total_count, unit_price = self.calculate_unit_price(title, price, desc_text)
                    tags = self.extract_tags(title + " " + desc_text)

                    # åŠ å…¥è³‡æ–™åˆ—è¡¨
                    self.data.append({
                        "source": "å¤§ç ”ç”Ÿé†«å®˜ç¶²",
                        "brand": "å¤§ç ”ç”Ÿé†«",
                        "title": title,
                        "price": price,
                        "unit_price": unit_price,
                        "url": link,
                        "image_url": image_url,
                        "product_highlights": "", # å¤§ç ”æš«ç„¡ AI åˆ†æ
                        "total_count": total_count,
                        "tags": tags
                    })

                except Exception as e:
                    print(f"âŒ æŠ“å–å¤±æ•— {link}: {e}")

            await browser.close()
            
            # æœ€å¾Œçµ±ä¸€å­˜æª”
            self.save_to_csv()

# è®“æ­¤æª”æ¡ˆå¯ç›´æ¥åŸ·è¡Œæ¸¬è©¦
if __name__ == "__main__":
    scraper = DaikenScraper()
    asyncio.run(scraper.run())
