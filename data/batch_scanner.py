import asyncio
import csv
import json
import os
import sys
import traceback
from datetime import datetime
from collections import defaultdict

import pandas as pd

# ç¢ºä¿å¯å¾å°ˆæ¡ˆæ ¹ç›®éŒ„åŒ¯å…¥æ¨¡çµ„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.sitemap_parser import SitemapParser
from data.agent_d2c_scanner import AgentD2CScanner


DOMAINS_CSV = "data/d2c_domains_list.csv"
TARGET_JSON = "data/target_product_urls.json"
OUTPUT_CSV = "data/d2c_full_database.csv"
ERROR_LOG = "data/batch_scanner_error.log"

TOP_N_BRANDS = 10
MAX_URLS_PER_BRAND = 30
MAX_RETRIES = 3
CONCURRENCY = 3

# --- å“ç‰Œé©—æ”¶é–€æª»èˆ‡ä»»å‹™æ©Ÿåˆ¶è¨­å®š ---
# ç›®æ¨™ï¼šç•¶å“ç‰ŒæŠ“å–æ•¸é¡¯è‘—ä½æ–¼é æœŸæ™‚ï¼Œè‡ªå‹•å»ºç«‹ã€Œæ‰¾å•é¡Œ/è§£å•é¡Œã€ä»»å‹™æ¸…å–®
EXPECTED_MIN_PRODUCTS = {
    "æ‚ æ´»åŸåŠ›": 50,  # äººå·¥é©—è­‰å®˜ç¶²ç´„ 50 é …
}

# è‹¥æŸå“ç‰Œç”¢å“æ•¸é æœŸè¼ƒé«˜ï¼Œå¯æ”¾å¯¬è©²å“ç‰Œ URL æƒæä¸Šé™
BRAND_URL_CAPS = {
    "æ‚ æ´»åŸåŠ›": 80,
}

ISSUE_TRACKER_DIR = "data/issue_tracker"


def log_error(stage, brand, url, err):
    os.makedirs("data", exist_ok=True)
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    msg = (
        f"[{ts}] stage={stage} brand={brand} url={url}\n"
        f"error={repr(err)}\n"
        f"traceback={traceback.format_exc()}\n"
        f"{'-' * 80}\n"
    )
    with open(ERROR_LOG, "a", encoding="utf-8") as f:
        f.write(msg)


def load_top_domains(path, top_n=10):
    domains = []
    with open(path, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            brand = (row.get("brand") or "").strip()
            domain = (row.get("domain") or "").strip()
            if brand and domain:
                domains.append((brand, domain))
    return domains[:top_n]


def save_to_csv(data, filepath):
    if not data:
        return

    schema = [
        "source",
        "brand",
        "title",
        "price",
        "unit_price",
        "total_count",
        "url",
        "image_url",
        "product_highlights",
    ]

    df_new = pd.DataFrame(data)
    for c in schema:
        if c not in df_new.columns:
            df_new[c] = ""
    df_new = df_new[schema]

    # è‹¥èˆŠæª”å­˜åœ¨å‰‡åˆä½µå»é‡
    if os.path.exists(filepath):
        try:
            df_old = pd.read_csv(filepath)
            for c in schema:
                if c not in df_old.columns:
                    df_old[c] = ""
            df_old = df_old[schema]
            df_all = pd.concat([df_old, df_new], ignore_index=True)
            if "url" in df_all.columns:
                df_all = df_all.drop_duplicates(subset=["url"], keep="last")
        except Exception:
            df_all = df_new
    else:
        df_all = df_new

    df_all.to_csv(filepath, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ å·²æ›´æ–°å­˜æª”: {filepath} (å…± {len(df_all)} ç­†)")


def build_issue_tasks(parse_metrics, success_metrics):
    """æ ¹æ“šå“ç‰Œè§£æ/æƒæçµæœï¼Œå»ºç«‹å¯åŸ·è¡Œä»»å‹™æ¸…å–®ã€‚"""
    issues = []

    for brand, stats in parse_metrics.items():
        expected = EXPECTED_MIN_PRODUCTS.get(brand)
        parsed = stats.get("parsed_urls", 0)
        capped = stats.get("capped_urls", 0)
        success = success_metrics.get(brand, 0)

        # 1) æ˜ç¢ºå°é½Šäººå·¥æœŸæœ›å€¼çš„å‘Šè­¦ï¼ˆä¾‹å¦‚ï¼šæ‚ æ´»åŸåŠ›æ‡‰æœ‰ 50 é …ï¼‰
        if expected is not None:
            if parsed < expected:
                issues.append({
                    "severity": "P0",
                    "brand": brand,
                    "stage": "sitemap_discovery",
                    "problem": f"Sitemap åƒ…è§£æåˆ° {parsed} å€‹ç”¢å“ URLï¼Œä½æ–¼ç›®æ¨™ {expected}",
                    "action": "æª¢æŸ¥ robots/sitemap å¯é”æ€§ã€ç«™å…§åˆ†é¡é  fallbackã€ç¶²å€éæ¿¾è¦å‰‡æ˜¯å¦éåš´",
                })
            if success < expected:
                issues.append({
                    "severity": "P0",
                    "brand": brand,
                    "stage": "extraction",
                    "problem": f"æœ€çµ‚åƒ…æˆåŠŸæŠ“å– {success} ç­†ï¼Œä½æ–¼ç›®æ¨™ {expected}",
                    "action": "é€æ­¥æ¯”å° target URLs èˆ‡ scan å¤±æ•—åŸå› ï¼Œè£œå¼· selector / é‡è©¦ / åçˆ¬è™•ç†",
                })

        # 2) æƒæè½‰æ›ç‡éä½å‘Šè­¦ï¼ˆé¿å…åªæ‰¾åˆ° URL å»æŠ“ä¸åˆ°å…§å®¹ï¼‰
        if capped > 0:
            hit_rate = success / capped
            if hit_rate < 0.5:
                issues.append({
                    "severity": "P1",
                    "brand": brand,
                    "stage": "extraction_quality",
                    "problem": f"URLâ†’æœ‰æ•ˆè³‡æ–™è½‰æ›ç‡åä½ ({success}/{capped}, {hit_rate:.1%})",
                    "action": "æŠ½æ¨£æª¢æŸ¥å¤±æ•— URLï¼Œå€åˆ†ã€é é¢ä¸å¯é” / selector å¤±æ•ˆ / LLM è§£æå¤±æ•—ã€å¾Œå®šå‘ä¿®å¾©",
                })


    return issues


def save_issue_tracker(parse_metrics, success_metrics, issues):
    os.makedirs(ISSUE_TRACKER_DIR, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_path = os.path.join(ISSUE_TRACKER_DIR, f"issues_{ts}.json")
    md_path = os.path.join(ISSUE_TRACKER_DIR, "latest_issues.md")

    payload = {
        "generated_at": datetime.now().isoformat(),
        "parse_metrics": parse_metrics,
        "success_metrics": success_metrics,
        "issues": issues,
    }

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    lines = [
        "# D2C å•é¡Œè¿½è¹¤ä»»å‹™æ¸…å–®",
        "",
        f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "## å“ç‰Œæƒææ‘˜è¦",
        "",
        "| å“ç‰Œ | è§£æURLæ•¸ | ç´å…¥æƒæURLæ•¸ | æˆåŠŸæŠ“å–ç­†æ•¸ |",
        "|---|---:|---:|---:|",
    ]

    for brand, stats in parse_metrics.items():
        lines.append(
            f"| {brand} | {stats.get('parsed_urls', 0)} | {stats.get('capped_urls', 0)} | {success_metrics.get(brand, 0)} |"
        )

    lines.extend(["", "## è‡ªå‹•ç”¢ç”Ÿä»»å‹™", ""])
    if not issues:
        lines.append("âœ… æœ¬è¼ªæœªç™¼ç¾éœ€è¦å‡ç´šè™•ç†çš„å“ç‰Œä»»å‹™ã€‚")
    else:
        for idx, item in enumerate(issues, 1):
            lines.append(
                f"{idx}. [{item['severity']}] {item['brand']} / {item['stage']}ï¼š{item['problem']}\n"
                f"   - å»ºè­°å‹•ä½œï¼š{item['action']}"
            )

    with open(md_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines) + "\n")

    return json_path, md_path


async def scan_url_with_retry(scanner, brand, url, max_retries=3):
    for attempt in range(1, max_retries + 1):
        try:
            result = await scanner.scan_url(url)
            if result:
                if not result.get("brand") or result.get("brand") == "Unknown":
                    result["brand"] = brand
                return result
            return None
        except Exception as e:
            log_error("scan_url", brand, url, e)
            if attempt < max_retries:
                wait_sec = min(2 ** attempt, 8)
                print(f"âš ï¸ [{brand}] URL é‡è©¦ {attempt}/{max_retries}: {url} ({wait_sec}s)")
                await asyncio.sleep(wait_sec)
            else:
                print(f"âŒ [{brand}] URL æœ€çµ‚å¤±æ•—: {url}")
    return None


async def main():
    os.makedirs("data", exist_ok=True)

    if not os.path.exists(DOMAINS_CSV):
        print(f"âŒ æ‰¾ä¸åˆ°ç¶²åŸŸæ¸…å–®: {DOMAINS_CSV}")
        return

    domains = load_top_domains(DOMAINS_CSV, TOP_N_BRANDS)
    print(f"ğŸ¯ ä»Šæ—¥ä»»å‹™ï¼šæƒæå‰ {len(domains)} å€‹å“ç‰Œ")
    for i, (brand, domain) in enumerate(domains, 1):
        print(f"  {i:02d}. {brand} -> {domain}")

    parser = SitemapParser()
    scanner = AgentD2CScanner()
    parse_metrics = {}

    # 1) å…ˆåš sitemap è§£æï¼ˆæ¯å€‹å“ç‰Œå¯é‡è©¦ï¼‰
    target_list = []
    for brand, domain in domains:
        parsed_ok = False
        for attempt in range(1, MAX_RETRIES + 1):
            try:
                items_all = parser.process_domain(brand, domain)
                cap = BRAND_URL_CAPS.get(brand, MAX_URLS_PER_BRAND)
                # æ¯å“ç‰Œé™åˆ¶å‰ N å€‹ï¼Œæ§æ™‚èˆ‡ç©©å®šï¼ˆå¯å“ç‰ŒåŒ–èª¿æ•´ï¼‰
                items = items_all[:cap]
                target_list.extend(items)
                parse_metrics[brand] = {
                    "domain": domain,
                    "parsed_urls": len(items_all),
                    "capped_urls": len(items),
                    "url_cap": cap,
                }
                parsed_ok = True
                break
            except Exception as e:
                log_error("parse_domain", brand, domain, e)
                if attempt < MAX_RETRIES:
                    wait_sec = min(2 ** attempt, 8)
                    print(f"âš ï¸ [{brand}] Sitemap é‡è©¦ {attempt}/{MAX_RETRIES} ({wait_sec}s)")
                    await asyncio.sleep(wait_sec)
                else:
                    print(f"âŒ [{brand}] Sitemap æœ€çµ‚å¤±æ•—ï¼Œç•¥é")
        if not parsed_ok:
            parse_metrics[brand] = {
                "domain": domain,
                "parsed_urls": 0,
                "capped_urls": 0,
                "url_cap": BRAND_URL_CAPS.get(brand, MAX_URLS_PER_BRAND),
            }
            continue

    # å­˜ target jsonï¼ˆæ–¹ä¾¿è¿½è¹¤ï¼‰
    with open(TARGET_JSON, "w", encoding="utf-8") as f:
        json.dump(target_list, f, ensure_ascii=False, indent=2)

    # URL å»é‡
    dedup_map = {}
    for item in target_list:
        u = item.get("url")
        b = item.get("brand", "Unknown")
        if u:
            dedup_map[u] = b
    pending = [{"url": u, "brand": b} for u, b in dedup_map.items()]

    if not pending:
        print("âš ï¸ æœ¬æ¬¡æ²’æœ‰å¯æƒæçš„ç”¢å“ URL")
        return

    print(f"ğŸ”— å¾…æƒæ URL æ•¸é‡: {len(pending)}")

    # 2) æƒæï¼ˆè‡ªå‹•é‡è©¦ + éŒ¯èª¤è¨˜éŒ„ + ä¸ä¸­æ–·ï¼‰
    sem = asyncio.Semaphore(CONCURRENCY)
    scanned_results = []
    success_metrics = defaultdict(int)

    async def _job(item):
        async with sem:
            res = await scan_url_with_retry(scanner, item["brand"], item["url"], MAX_RETRIES)
            if res:
                scanned_results.append(res)
                b = (res.get("brand") or item["brand"] or "Unknown").strip()
                success_metrics[b] += 1

    await asyncio.gather(*[_job(it) for it in pending])

    # 3) è¼¸å‡º
    save_to_csv(scanned_results, OUTPUT_CSV)

    # 4) å•é¡Œè¿½è¹¤èˆ‡è§£é¡Œä»»å‹™æ¸…å–®
    issue_tasks = build_issue_tasks(parse_metrics, success_metrics)
    issue_json, issue_md = save_issue_tracker(parse_metrics, success_metrics, issue_tasks)

    print("\nâœ… ä»»å‹™å®Œæˆ")
    print(f"- ç›®æ¨™å“ç‰Œæ•¸: {len(domains)}")
    print(f"- æå–ç›®æ¨™ URL: {len(pending)}")
    print(f"- æˆåŠŸæŠ“å–ç­†æ•¸: {len(scanned_results)}")
    print(f"- Error Log: {ERROR_LOG}")
    print(f"- å•é¡Œè¿½è¹¤(JSON): {issue_json}")
    print(f"- å•é¡Œè¿½è¹¤(MD): {issue_md}")


if __name__ == "__main__":
    asyncio.run(main())
