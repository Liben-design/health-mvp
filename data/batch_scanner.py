import asyncio
import csv
import json
import os
import sys
import traceback
from datetime import datetime

import pandas as pd

# ç¢ºä¿å¯å¾å°ˆæ¡ˆæ ¹ç›®éŒ„åŒ¯å…¥æ¨¡çµ„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.sitemap_parser import SitemapParser
from data.agent_d2c_scanner import AgentD2CScanner


DOMAINS_CSV = "data/d2c_domains_list.csv"
TARGET_JSON = "data/target_product_urls.json"
OUTPUT_CSV = "data/d2c_full_database.csv"
ERROR_LOG = "data/batch_scanner_error.log"

TOP_N_BRANDS = 10
MAX_URLS_PER_BRAND = 30
MAX_RETRIES = 3
CONCURRENCY = 3


def log_error(stage, brand, url, err):
    os.makedirs("data", exist_ok=True)
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    msg = (
        f"[{ts}] stage={stage} brand={brand} url={url}\n"
        f"error={repr(err)}\n"
        f"traceback={traceback.format_exc()}\n"
        f"{'-' * 80}\n"
    )
    with open(ERROR_LOG, "a", encoding="utf-8") as f:
        f.write(msg)


def load_top_domains(path, top_n=10):
    domains = []
    with open(path, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            brand = (row.get("brand") or "").strip()
            domain = (row.get("domain") or "").strip()
            if brand and domain:
                domains.append((brand, domain))
    return domains[:top_n]


def save_to_csv(data, filepath):
    if not data:
        return

    schema = [
        "source",
        "brand",
        "title",
        "price",
        "unit_price",
        "total_count",
        "url",
        "image_url",
        "product_highlights",
    ]

    df_new = pd.DataFrame(data)
    for c in schema:
        if c not in df_new.columns:
            df_new[c] = ""
    df_new = df_new[schema]

    # è‹¥èˆŠæª”å­˜åœ¨å‰‡åˆä½µå»é‡
    if os.path.exists(filepath):
        try:
            df_old = pd.read_csv(filepath)
            for c in schema:
                if c not in df_old.columns:
                    df_old[c] = ""
            df_old = df_old[schema]
            df_all = pd.concat([df_old, df_new], ignore_index=True)
            if "url" in df_all.columns:
                df_all = df_all.drop_duplicates(subset=["url"], keep="last")
        except Exception:
            df_all = df_new
    else:
        df_all = df_new

    df_all.to_csv(filepath, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ å·²æ›´æ–°å­˜æª”: {filepath} (å…± {len(df_all)} ç­†)")


async def scan_url_with_retry(scanner, brand, url, max_retries=3):
    for attempt in range(1, max_retries + 1):
        try:
            result = await scanner.scan_url(url)
            if result:
                if not result.get("brand") or result.get("brand") == "Unknown":
                    result["brand"] = brand
                return result
            return None
        except Exception as e:
            log_error("scan_url", brand, url, e)
            if attempt < max_retries:
                wait_sec = min(2 ** attempt, 8)
                print(f"âš ï¸ [{brand}] URL é‡è©¦ {attempt}/{max_retries}: {url} ({wait_sec}s)")
                await asyncio.sleep(wait_sec)
            else:
                print(f"âŒ [{brand}] URL æœ€çµ‚å¤±æ•—: {url}")
    return None


async def main():
    os.makedirs("data", exist_ok=True)

    if not os.path.exists(DOMAINS_CSV):
        print(f"âŒ æ‰¾ä¸åˆ°ç¶²åŸŸæ¸…å–®: {DOMAINS_CSV}")
        return

    domains = load_top_domains(DOMAINS_CSV, TOP_N_BRANDS)
    print(f"ğŸ¯ ä»Šæ—¥ä»»å‹™ï¼šæƒæå‰ {len(domains)} å€‹å“ç‰Œ")
    for i, (brand, domain) in enumerate(domains, 1):
        print(f"  {i:02d}. {brand} -> {domain}")

    parser = SitemapParser()
    scanner = AgentD2CScanner()

    # 1) å…ˆåš sitemap è§£æï¼ˆæ¯å€‹å“ç‰Œå¯é‡è©¦ï¼‰
    target_list = []
    for brand, domain in domains:
        parsed_ok = False
        for attempt in range(1, MAX_RETRIES + 1):
            try:
                items = parser.process_domain(brand, domain)
                # æ¯å“ç‰Œé™åˆ¶å‰ N å€‹ï¼Œæ§æ™‚èˆ‡ç©©å®š
                items = items[:MAX_URLS_PER_BRAND]
                target_list.extend(items)
                parsed_ok = True
                break
            except Exception as e:
                log_error("parse_domain", brand, domain, e)
                if attempt < MAX_RETRIES:
                    wait_sec = min(2 ** attempt, 8)
                    print(f"âš ï¸ [{brand}] Sitemap é‡è©¦ {attempt}/{MAX_RETRIES} ({wait_sec}s)")
                    await asyncio.sleep(wait_sec)
                else:
                    print(f"âŒ [{brand}] Sitemap æœ€çµ‚å¤±æ•—ï¼Œç•¥é")
        if not parsed_ok:
            continue

    # å­˜ target jsonï¼ˆæ–¹ä¾¿è¿½è¹¤ï¼‰
    with open(TARGET_JSON, "w", encoding="utf-8") as f:
        json.dump(target_list, f, ensure_ascii=False, indent=2)

    # URL å»é‡
    dedup_map = {}
    for item in target_list:
        u = item.get("url")
        b = item.get("brand", "Unknown")
        if u:
            dedup_map[u] = b
    pending = [{"url": u, "brand": b} for u, b in dedup_map.items()]

    if not pending:
        print("âš ï¸ æœ¬æ¬¡æ²’æœ‰å¯æƒæçš„ç”¢å“ URL")
        return

    print(f"ğŸ”— å¾…æƒæ URL æ•¸é‡: {len(pending)}")

    # 2) æƒæï¼ˆè‡ªå‹•é‡è©¦ + éŒ¯èª¤è¨˜éŒ„ + ä¸ä¸­æ–·ï¼‰
    sem = asyncio.Semaphore(CONCURRENCY)
    scanned_results = []

    async def _job(item):
        async with sem:
            res = await scan_url_with_retry(scanner, item["brand"], item["url"], MAX_RETRIES)
            if res:
                scanned_results.append(res)

    await asyncio.gather(*[_job(it) for it in pending])

    # 3) è¼¸å‡º
    save_to_csv(scanned_results, OUTPUT_CSV)

    print("\nâœ… ä»»å‹™å®Œæˆ")
    print(f"- ç›®æ¨™å“ç‰Œæ•¸: {len(domains)}")
    print(f"- æå–ç›®æ¨™ URL: {len(pending)}")
    print(f"- æˆåŠŸæŠ“å–ç­†æ•¸: {len(scanned_results)}")
    print(f"- Error Log: {ERROR_LOG}")


if __name__ == "__main__":
    asyncio.run(main())
