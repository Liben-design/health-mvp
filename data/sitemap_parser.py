import requests
import xml.etree.ElementTree as ET
import csv
import json
import os
import re
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

class SitemapParser:
    """
    è¼•é‡åŒ– Sitemap è§£æå™¨ (Phase 2 Core Module)
    ä¸ä¾è³´ç€è¦½å™¨ï¼Œä½¿ç”¨ Requests èˆ‡ XML Parser å¿«é€Ÿæå–ç”¢å“é€£çµã€‚
    """
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            # æ”¹ç”¨ä¸€èˆ¬ç€è¦½å™¨ UAï¼Œé™ä½è¢«é˜²ç«ç‰†é˜»æ“‹æ©Ÿç‡ (è§£æ±ºé…æ–¹æ™‚ä»£ç­‰ç¶²ç«™é€£ç·šå•é¡Œ)
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        # é—œéµéæ¿¾ï¼šç¶²å€å¿…é ˆåŒ…å«é€™äº›ç‰¹å¾µä¹‹ä¸€
        # æ–°å¢ 'product.php' æ”¯æ´å¤§ç ”ç”Ÿé†«
        self.include_patterns = ['/product/', '/products/', '/item/', '/goods/', '/merch/', '/shop/', 'product.php']
        
        # [New] é‡å°ç‰¹å®šç¶²åŸŸæ”¾å¯¬éæ¿¾æ¨™æº– (å¦‚é…æ–¹æ™‚ä»£ä½¿ç”¨è‡ªå®šç¾© URLï¼Œä¸å« product å‰ç¶´)
        self.relaxed_domains = ['healthformula.com.tw']

        # æ’é™¤é›œè¨Šï¼šç¶²å€ä¸èƒ½åŒ…å«é€™äº›ç‰¹å¾µ
        # æ–°å¢ 'knowledge', 'about' ç­‰å¸¸è¦‹éç”¢å“é é¢
        self.exclude_patterns = ['/blog', '/news', '/article', '/page', '/about', '/contact', '/faq', '/terms', 
                                 '/collections/', '/category/', '/tag/', '/knowledge/', '/media/', '/policy/', '/account/', '/cart/', '/member/']

    def fetch_content(self, url):
        """è¼•é‡åŒ–æŠ“å–å…§å®¹ï¼Œå«è¶…æ™‚æ§åˆ¶"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                return response.text
        except Exception as e:
            # éœé»˜å¤±æ•—ï¼Œåƒ…åœ¨ debug æ™‚è¼¸å‡º
            # print(f"âš ï¸ é€£ç·šå¤±æ•— {url}: {e}")
            pass
        return None

    def get_sitemaps_from_robots(self, domain):
        """å¾ robots.txt å°‹æ‰¾ Sitemap å®£å‘Š"""
        robots_url = urljoin(domain, "/robots.txt")
        content = self.fetch_content(robots_url)
        sitemaps = []
        if content:
            for line in content.splitlines():
                if line.lower().startswith("sitemap:"):
                    try:
                        sitemap_url = line.split(":", 1)[1].strip()
                        sitemaps.append(sitemap_url)
                    except: pass
        return sitemaps

    def is_likely_product(self, url):
        """ç¶²å€éæ¿¾é‚è¼¯ï¼šåªä¿ç•™ç”¢å“é """
        u = url.lower()
        
        # [New] 0. å…¨åŸŸæ¸…æ´—è¦å‰‡ (é‡å°æ‚ æ´»åŸåŠ›ç­‰)
        # æ’é™¤é ASCII (äº‚ç¢¼/ä¸­æ–‡è·¯å¾‘)
        if re.search(r'[^\x00-\x7F]', u):
            return False
        # æ’é™¤çµå°¾é•·æ•¸å­— (æ™‚é–“æˆ³è¨˜/è®Šé«”) e.g. -20220719115000
        if re.search(r'-\d{6,}', u):
            return False

        # 1. æ’é™¤ç‰¹å¾µ (å„ªå…ˆåŸ·è¡Œ)
        if any(ex in u for ex in self.exclude_patterns):
            return False

        # [New] 2. å¯¬é¬†ç¶²åŸŸæª¢æŸ¥ (è·³éåŒ…å«ç‰¹å¾µæª¢æŸ¥)
        for domain in self.relaxed_domains:
            if domain in u:
                return True

        # 3. å¿…é ˆåŒ…å«ç”¢å“ç‰¹å¾µ
        if not any(p in u for p in self.include_patterns):
            return False
        return True

    def parse_xml(self, xml_content):
        """è§£æ XML ä¸¦è™•ç† Namespace å•é¡Œ"""
        try:
            # ç§»é™¤ xmlns å±¬æ€§ä»¥ç°¡åŒ– ElementTree çš„æŸ¥æ‰¾ (é¿å…è™•ç†è¤‡é›œçš„ namespace map)
            xml_content = re.sub(r'xmlns="[^"]+"', '', xml_content, count=1)
            root = ET.fromstring(xml_content)
            return root
        except ET.ParseError:
            return None

    def process_domain(self, brand, domain):
        """è™•ç†å–®ä¸€ç¶²åŸŸçš„å®Œæ•´æµç¨‹ï¼šRobots -> Sitemap -> URLs"""
        print(f"ğŸ” [Sitemap] é–‹å§‹æƒæ: {brand} ({domain})")
        found_urls = set()
        total_scanned = 0
        
        # 1. æ”¶é›†ç¨®å­ Sitemaps
        sitemap_queue = self.get_sitemaps_from_robots(domain)
        if not sitemap_queue:
            # Fallback: è‹¥ robots.txt æ²’å¯«ï¼Œå˜—è©¦å¸¸è¦‹è·¯å¾‘
            defaults = [
                "/sitemap.xml",
                "/sitemap_index.xml",
                "/sitemap_products_1.xml", # Shopify å¸¸è¦‹
                "/wp-sitemap.xml" # WordPress 5.5+ é è¨­
            ]
            sitemap_queue = [urljoin(domain, p) for p in defaults]

        processed_sitemaps = set()

        # 2. éè¿´è§£æ (å»£åº¦å„ªå…ˆæœå°‹)
        while sitemap_queue:
            current_sitemap = sitemap_queue.pop(0)
            if current_sitemap in processed_sitemaps:
                continue
            processed_sitemaps.add(current_sitemap)

            xml_content = self.fetch_content(current_sitemap)
            if not xml_content:
                continue

            root = self.parse_xml(xml_content)
            if root is None:
                continue

            # A. è™•ç† Sitemap Index (å·¢ç‹€ Sitemap)
            # æ ¼å¼: <sitemap><loc>...</loc></sitemap>
            for sitemap in root.findall(".//sitemap"):
                loc = sitemap.find("loc")
                if loc is not None and loc.text:
                    sitemap_queue.append(loc.text.strip())

            # B. è™•ç† URL Set (å¯¦éš›é€£çµ)
            # æ ¼å¼: <url><loc>...</loc></url>
            for url_tag in root.findall(".//url"):
                loc = url_tag.find("loc")
                if loc is not None and loc.text:
                    url = loc.text.strip()
                    total_scanned += 1
                    if self.is_likely_product(url):
                        found_urls.add(url)

        filter_rate = (1 - len(found_urls) / total_scanned) * 100 if total_scanned > 0 else 0
        print(f"âœ… [Sitemap] {brand} å®Œæˆï¼Œæƒæ {total_scanned} é€£çµ -> æå– {len(found_urls)} ç”¢å“ (éæ¿¾ç‡ {filter_rate:.1f}%)")
        return [{"brand": brand, "url": u} for u in found_urls]

def main():
    input_csv = "data/d2c_domains_list.csv"
    output_json = "data/target_product_urls.json"
    
    # æª¢æŸ¥è¼¸å…¥æª”
    if not os.path.exists(input_csv):
        print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {input_csv}ï¼Œè«‹å…ˆå»ºç«‹å“ç‰Œæ¸…å–®ã€‚")
        return

    results = []
    parser = SitemapParser()
    domains = []

    # è®€å– CSV
    with open(input_csv, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row.get("domain") and row.get("brand"):
                domains.append((row["brand"], row["domain"]))

    # [æ¸¬è©¦æ¨¡å¼] åƒ…è™•ç†å‰ 5 å€‹å“ç‰Œé€²è¡Œæ ¡æº–
    print(f"âš ï¸ æ¸¬è©¦æ¨¡å¼å•Ÿå‹•ï¼šåƒ…è™•ç†æ¸…å–®ä¸­çš„å‰ 5 å€‹å“ç‰Œ (å…± {len(domains)} å€‹)")
    domains = domains[:5]

    # å¹³è¡Œè™•ç† (åŠ é€Ÿ)
    print(f"ğŸš€ å•Ÿå‹• Sitemap è§£æå™¨ï¼Œå…± {len(domains)} å€‹ç›®æ¨™...")
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_brand = {executor.submit(parser.process_domain, b, d): b for b, d in domains}
        for future in as_completed(future_to_brand):
            results.extend(future.result())

    # è¼¸å‡ºçµæœ
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ è§£æå®Œæˆï¼å…±æ”¶é›† {len(results)} å€‹ç”¢å“é€£çµï¼Œå·²å„²å­˜è‡³ {output_json}")

if __name__ == "__main__":
    main()