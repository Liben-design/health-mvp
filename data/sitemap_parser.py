import requests
import xml.etree.ElementTree as ET
import csv
import json
import os
import re
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

class SitemapParser:
    """
    è¼•é‡åŒ– Sitemap è§£æå™¨ (Phase 2 Core Module)
    ä¸ä¾è³´ç€è¦½å™¨ï¼Œä½¿ç”¨ Requests èˆ‡ XML Parser å¿«é€Ÿæå–ç”¢å“é€£çµã€‚
    """
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            # æ”¹ç”¨ä¸€èˆ¬ç€è¦½å™¨ UAï¼Œé™ä½è¢«é˜²ç«ç‰†é˜»æ“‹æ©Ÿç‡ (è§£æ±ºé…æ–¹æ™‚ä»£ç­‰ç¶²ç«™é€£ç·šå•é¡Œ)
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        # é—œéµéæ¿¾ï¼šç¶²å€å¿…é ˆåŒ…å«é€™äº›ç‰¹å¾µä¹‹ä¸€
        # æ–°å¢ 'product.php' æ”¯æ´å¤§ç ”ç”Ÿé†«
        self.include_patterns = ['/product/', '/products/', '/item/', '/goods/', '/merch/', '/shop/', 'product.php']
        
        # [New] é‡å°ç‰¹å®šç¶²åŸŸæ”¾å¯¬éæ¿¾æ¨™æº– (å¦‚é…æ–¹æ™‚ä»£ä½¿ç”¨è‡ªå®šç¾© URLï¼Œä¸å« product å‰ç¶´)
        self.relaxed_domains = ['healthformula.com.tw']
        
        # [New] ç¶²åŸŸç™½åå–®è¦å‰‡ï¼šå¯è¦†è“‹ä¸€èˆ¬ include/exclude é‚è¼¯
        # é…æ–¹æ™‚ä»£ (formula-time) ç”¢å“ URL çµæ§‹å¯èƒ½ä¸å›ºå®šï¼Œä¸”æœ‰æ™‚ä¸å« /product/
        # å…è¨±çŸ­è·¯å¾‘ slug (e.g. /lutein-ex)ï¼Œä½†ä»æ’é™¤å¸¸è¦‹å…§å®¹é 
        self.domain_whitelist_rules = {
            "www.95dan.com.tw": {
                "allow_patterns": [
                    "/alcohol-enzyme",
                    "/maca",
                    "/macaplus",
                    "/lutein",
                    "/b+zinc",
                    "/b+fe",
                    "/arginine",
                    "/pumpkin",
                    "/withania",
                    "/curcumin",
                    "/fishoil",
                    "/calcium",
                    "/calciumplus",
                    "/collagen",
                    "/vitaminc",
                    "/vitamine",
                    "/probiotics",
                    "/fiberplus",
                    "/cranberry",
                    "/dmannose",
                    "/gsh-enzyme",
                    "/gaba-enzyme",
                    "/simply-enzyme",
                    "/superhca",
                    "/tryptophan",
                    "/polypeptide-p",
                    "/pct2",
                    "/biotin",
                    "/msm"
                ],
                "deny_patterns": [
                    "/allproduct",
                    "/home",
                    "/about",
                    "/aboutus",
                    "/news",
                    "/blog",
                    "/media",
                    "/kol",
                    "/corporate",
                    "/shippingpolicy",
                    "/refund",
                    "/signin",
                    "/sgs",
                    "/shopee",
                    "/terms",
                    "/policy",
                    "/privacy",
                    "/contact"
                ],
                "allow_short_slug": False
            },
            "www.formula-time.com": {
                "allow_patterns": [
                    "/products/",
                    "/product/",
                    "/shop/",
                    "lutein",
                    "fish-oil",
                    "probiotic",
                    "omega",
                    "collagen",
                    "calcium",
                    "magnesium",
                    "vitamin",
                    "b-complex",
                    "zinc",
                    "iron"
                ],
                "deny_patterns": [
                    "/blog",
                    "/news",
                    "/article",
                    "/about",
                    "/contact",
                    "/faq",
                    "/policy",
                    "/member",
                    "/cart",
                    "/account",
                    "/terms",
                    "/privacy",
                    "/page",
                    "/pages"
                ],
                "allow_short_slug": True
            },
            "formula-time.com": {
                "allow_patterns": [
                    "/products/",
                    "/product/",
                    "/shop/",
                    "lutein",
                    "fish-oil",
                    "probiotic",
                    "omega",
                    "collagen",
                    "calcium",
                    "magnesium",
                    "vitamin",
                    "b-complex",
                    "zinc",
                    "iron"
                ],
                "deny_patterns": [
                    "/blog",
                    "/news",
                    "/article",
                    "/about",
                    "/contact",
                    "/faq",
                    "/policy",
                    "/member",
                    "/cart",
                    "/account",
                    "/terms",
                    "/privacy",
                    "/page",
                    "/pages"
                ],
                "allow_short_slug": True
            }
        }

        # æ’é™¤é›œè¨Šï¼šç¶²å€ä¸èƒ½åŒ…å«é€™äº›ç‰¹å¾µ
        # æ–°å¢ 'knowledge', 'about' ç­‰å¸¸è¦‹éç”¢å“é é¢
        self.exclude_patterns = ['/blog', '/news', '/article', '/page', '/about', '/contact', '/faq', '/terms', 
                                 '/collections/', '/category/', '/tag/', '/knowledge/', '/media/', '/policy/', '/account/', '/cart/', '/member/']

    def fetch_content(self, url):
        """è¼•é‡åŒ–æŠ“å–å…§å®¹ï¼Œå«è¶…æ™‚æ§åˆ¶"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                return response.text
        except Exception as e:
            # éœé»˜å¤±æ•—ï¼Œåƒ…åœ¨ debug æ™‚è¼¸å‡º
            # print(f"âš ï¸ é€£ç·šå¤±æ•— {url}: {e}")
            pass
        return None

    def get_sitemaps_from_robots(self, domain):
        """å¾ robots.txt å°‹æ‰¾ Sitemap å®£å‘Š"""
        robots_url = urljoin(domain, "/robots.txt")
        content = self.fetch_content(robots_url)
        sitemaps = []
        if content:
            for line in content.splitlines():
                if line.lower().startswith("sitemap:"):
                    try:
                        sitemap_url = line.split(":", 1)[1].strip()
                        sitemaps.append(sitemap_url)
                    except: pass
        return sitemaps

    def is_likely_product(self, url):
        """ç¶²å€éæ¿¾é‚è¼¯ï¼šåªä¿ç•™ç”¢å“é """
        u = url.lower()
        parsed = urlparse(u)
        host = parsed.netloc
        
        # [New] 0. å…¨åŸŸæ¸…æ´—è¦å‰‡ (é‡å°æ‚ æ´»åŸåŠ›ç­‰)
        # æ’é™¤é ASCII (äº‚ç¢¼/ä¸­æ–‡è·¯å¾‘)
        if re.search(r'[^\x00-\x7F]', u):
            return False
        # æ’é™¤çµå°¾é•·æ•¸å­— (æ™‚é–“æˆ³è¨˜/è®Šé«”) e.g. -20220719115000
        if re.search(r'-\d{6,}', u):
            return False

        # [New] 0.5 ç¶²åŸŸç™½åå–®è¦å‰‡ (é…æ–¹æ™‚ä»£å°ˆç”¨)
        if host in self.domain_whitelist_rules:
            rules = self.domain_whitelist_rules[host]
            if any(deny in u for deny in rules["deny_patterns"]):
                return False
            if any(allow in u for allow in rules["allow_patterns"]):
                return True
            if rules.get("allow_short_slug"):
                path = parsed.path.strip("/")
                # å…è¨±å–®ä¸€ slug ä¸”é•·åº¦è¶³å¤ ï¼ˆé¿å… /about é€™é¡é é¢ï¼‰
                if path and "/" not in path and len(path) >= 5:
                    return True
            # ç™½åå–®ç¶²åŸŸä½†æ²’æœ‰æ˜é¡¯ç”¢å“ç‰¹å¾µæ™‚ï¼Œä¿å®ˆä¸æ”¾è¡Œ
            return False

        # 1. æ’é™¤ç‰¹å¾µ (å„ªå…ˆåŸ·è¡Œ)
        if any(ex in u for ex in self.exclude_patterns):
            return False

        # [New] 2. å¯¬é¬†ç¶²åŸŸæª¢æŸ¥ (è·³éåŒ…å«ç‰¹å¾µæª¢æŸ¥)
        for domain in self.relaxed_domains:
            if domain in u:
                return True

        # 3. å¿…é ˆåŒ…å«ç”¢å“ç‰¹å¾µ
        if not any(p in u for p in self.include_patterns):
            return False
        return True

    def parse_xml(self, xml_content):
        """è§£æ XML ä¸¦è™•ç† Namespace å•é¡Œ"""
        try:
            # ç§»é™¤ xmlns å±¬æ€§ä»¥ç°¡åŒ– ElementTree çš„æŸ¥æ‰¾ (é¿å…è™•ç†è¤‡é›œçš„ namespace map)
            xml_content = re.sub(r'xmlns="[^"]+"', '', xml_content, count=1)
            root = ET.fromstring(xml_content)
            return root
        except ET.ParseError:
            return None

    def process_domain(self, brand, domain):
        """è™•ç†å–®ä¸€ç¶²åŸŸçš„å®Œæ•´æµç¨‹ï¼šRobots -> Sitemap -> URLs"""
        print(f"ğŸ” [Sitemap] é–‹å§‹æƒæ: {brand} ({domain})")
        found_urls = set()
        total_scanned = 0
        
        # 1. æ”¶é›†ç¨®å­ Sitemaps
        sitemap_queue = self.get_sitemaps_from_robots(domain)
        if not sitemap_queue:
            # Fallback: è‹¥ robots.txt æ²’å¯«ï¼Œå˜—è©¦å¸¸è¦‹è·¯å¾‘
            defaults = [
                "/sitemap.xml",
                "/sitemap_index.xml",
                "/sitemap_products_1.xml", # Shopify å¸¸è¦‹
                "/wp-sitemap.xml" # WordPress 5.5+ é è¨­
            ]
            sitemap_queue = [urljoin(domain, p) for p in defaults]

        processed_sitemaps = set()

        # 2. éè¿´è§£æ (å»£åº¦å„ªå…ˆæœå°‹)
        while sitemap_queue:
            current_sitemap = sitemap_queue.pop(0)
            if current_sitemap in processed_sitemaps:
                continue
            processed_sitemaps.add(current_sitemap)

            xml_content = self.fetch_content(current_sitemap)
            if not xml_content:
                continue

            root = self.parse_xml(xml_content)
            if root is None:
                continue

            # A. è™•ç† Sitemap Index (å·¢ç‹€ Sitemap)
            # æ ¼å¼: <sitemap><loc>...</loc></sitemap>
            for sitemap in root.findall(".//sitemap"):
                loc = sitemap.find("loc")
                if loc is not None and loc.text:
                    sitemap_queue.append(loc.text.strip())

            # B. è™•ç† URL Set (å¯¦éš›é€£çµ)
            # æ ¼å¼: <url><loc>...</loc></url>
            for url_tag in root.findall(".//url"):
                loc = url_tag.find("loc")
                if loc is not None and loc.text:
                    url = loc.text.strip()
                    total_scanned += 1
                    if self.is_likely_product(url):
                        found_urls.add(url)

        # ä¹äº”ä¹‹ä¸¹è£œå¼·ï¼šå¾ç”¢å“ç¸½è¦½é è£œæŠ“ç”¢å“è©³æƒ…é€£çµï¼ˆå›ºå®šåŸ·è¡Œï¼‰ï¼Œé¿å… sitemap æ¬„ä½ä¸è¶³
        if "95dan.com.tw" in domain:
            all_product_page = urljoin(domain, "/allproduct")
            html = self.fetch_content(all_product_page)
            if html:
                hrefs = re.findall(r'href=["\']([^"\']+)["\']', html)
                for href in hrefs:
                    full_url = urljoin(domain, href)
                    if self.is_likely_product(full_url):
                        found_urls.add(full_url)

            # 95dan è£œå¼·ï¼šæœ‰äº›ç”¢å“å¡ç‰‡ç”±å‰ç«¯æ¸²æŸ“ï¼Œç›´æ¥ä»¥ç™½åå–® slug åˆæˆ URL è£œé½Š
            rule = self.domain_whitelist_rules.get("www.95dan.com.tw", {})
            for token in rule.get("allow_patterns", []):
                if token.startswith("/"):
                    candidate = urljoin(domain, token)
                    if self.is_likely_product(candidate):
                        found_urls.add(candidate)

        filter_rate = (1 - len(found_urls) / total_scanned) * 100 if total_scanned > 0 else 0
        print(f"âœ… [Sitemap] {brand} å®Œæˆï¼Œæƒæ {total_scanned} é€£çµ -> æå– {len(found_urls)} ç”¢å“ (éæ¿¾ç‡ {filter_rate:.1f}%)")
        return [{"brand": brand, "url": u} for u in found_urls]

def main():
    input_csv = "data/test_domains.csv"
    output_json = "data/target_product_urls.json"
    
    # æª¢æŸ¥è¼¸å…¥æª”
    if not os.path.exists(input_csv):
        print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {input_csv}ï¼Œè«‹å…ˆå»ºç«‹å“ç‰Œæ¸…å–®ã€‚")
        return

    results = []
    parser = SitemapParser()
    domains = []

    # è®€å– CSV
    with open(input_csv, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row.get("domain") and row.get("brand"):
                domains.append((row["brand"], row["domain"]))

    # [æ¸¬è©¦æ¨¡å¼] åƒ…è™•ç†å‰ 5 å€‹å“ç‰Œé€²è¡Œæ ¡æº–
    print(f"âš ï¸ æ¸¬è©¦æ¨¡å¼å•Ÿå‹•ï¼šåƒ…è™•ç†æ¸…å–®ä¸­çš„å‰ 5 å€‹å“ç‰Œ (å…± {len(domains)} å€‹)")
    domains = domains[:5]

    # å¹³è¡Œè™•ç† (åŠ é€Ÿ)
    print(f"ğŸš€ å•Ÿå‹• Sitemap è§£æå™¨ï¼Œå…± {len(domains)} å€‹ç›®æ¨™...")
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_brand = {executor.submit(parser.process_domain, b, d): b for b, d in domains}
        for future in as_completed(future_to_brand):
            results.extend(future.result())

    # è¼¸å‡ºçµæœ
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ è§£æå®Œæˆï¼å…±æ”¶é›† {len(results)} å€‹ç”¢å“é€£çµï¼Œå·²å„²å­˜è‡³ {output_json}")

if __name__ == "__main__":
    main()