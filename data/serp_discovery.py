import asyncio
import re
import random
from urllib.parse import urlparse
from playwright.async_api import async_playwright
try:
    from playwright_stealth import stealth_async
except Exception:
    # Fallback when playwright_stealth is not available in the environment
    async def stealth_async(page):
        return None

class SerpDiscovery:
    """
    SERP (Search Engine Results Page) åµå¯Ÿæ¨¡çµ„
    è² è²¬æœå°‹é—œéµå­—ä¸¦éæ¿¾å‡ºæ½›åœ¨çš„ D2C å“ç‰Œå®˜ç¶²ã€‚
    """
    def __init__(self):
        # é»‘åå–®ï¼šæ’é™¤é›»å•†å¹³å°ã€åª’é«”ã€è«–å£‡ã€æ”¿åºœæ©Ÿæ§‹ç­‰é D2C ç¶²ç«™
        self.blacklisted_domains = [
            "momo.com.tw", "pchome.com.tw", "shopee.tw", "yahoo.com", 
            "yahoo.com.tw", "books.com.tw", "rakuten.com.tw", "etmall.com.tw", "friDay.tw",
            "biggo.com.tw", "feebee.com.tw",
            "ptt.cc", "dcard.tw", "mobile01.com", "pixnet.net", "canceraway.com",
            "facebook.com", "instagram.com", "youtube.com", "wikipedia.org",
            "gov.tw", "edu.tw", "commonhealth.com.tw", "heho.com.tw",
            "edh.tw", "health.udn.com", "top1health.com"
        ]
        # éš¨æ©Ÿ User-Agent åˆ—è¡¨
        self.user_agents = [
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
        ]

    def is_valid_d2c_domain(self, url):
        """SmartFilter: åˆ¤æ–·æ˜¯å¦ç‚ºæ½›åœ¨çš„ D2C å®˜ç¶²"""
        try:
            domain = urlparse(url).netloc.lower()
            # ç§»é™¤ www. å‰ç¶´ä»¥ä¾¿æ¯”å°
            if domain.startswith("www."):
                domain = domain[4:]
            
            # æª¢æŸ¥é»‘åå–®
            for blocked in self.blacklisted_domains:
                if blocked in domain:
                    return False
            return True
        except:
            return False

    async def search_google(self, keyword, pages=10, results_per_page=10):
        """
        ä½¿ç”¨ Playwright æ¨¡æ“¬ç€è¦½å™¨æœå°‹ Googleï¼Œè¦é¿ç°¡å–®çš„çˆ¬èŸ²æª¢æ¸¬ã€‚
        """
        print(f"ğŸ•µï¸ [SERP] æ­£åœ¨æœå°‹: {keyword} ...")
        results = set()
        
        async with async_playwright() as p:
            # åŠ å…¥ args é™ä½è¢«è‡ªå‹•åŒ–åµæ¸¬çš„æ©Ÿç‡
            # æ”¹ç‚º headless=False (æœ‰é ­æ¨¡å¼)ï¼Œè®“ç€è¦½å™¨è¦–çª—å½ˆå‡ºï¼Œå¤§å¹…é™ä½è¢« Google å°é–æ©Ÿç‡ï¼Œä¸¦å…è¨±äººå·¥é©—è­‰
            browser = await p.chromium.launch(headless=False, args=["--disable-blink-features=AutomationControlled"])
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent=random.choice(self.user_agents)
            )
            page = await context.new_page()
            await stealth_async(page)

            try:
                for page_index in range(pages):
                    start = page_index * results_per_page
                    # å‰å¾€ Google æœå°‹
                    # åŠ å…¥ hl=zh-TW å¼·åˆ¶ä¸­æ–‡ä»‹é¢ï¼Œé¿å…çµæ§‹å·®ç•°
                    await page.goto(
                        f"https://www.google.com/search?q={keyword}&num={results_per_page}&start={start}&hl=zh-TW",
                        wait_until="domcontentloaded"
                    )
                    
                    # ç­‰å¾…æœå°‹çµæœå®¹å™¨å‡ºç¾ (æœ€å¤šç­‰ 8 ç§’)
                    try:
                        await page.wait_for_selector("#search", timeout=8000)
                    except:
                        print(f"âš ï¸ [SERP] ç­‰å¾…æœå°‹çµæœè¶…æ™‚ï¼Œå¯èƒ½é‡åˆ° Captcha")
                        print("â³ åµæ¸¬åˆ°ç•°å¸¸ï¼Œæš«åœ 20 ç§’ä¾›äººå·¥æ’é™¤ (è«‹åœ¨å½ˆå‡ºçš„ç€è¦½å™¨è¦–çª—ä¸­å®Œæˆé©—è­‰)...")
                        # çµ¦äºˆäººå·¥é©—è­‰æ™‚é–“
                        await asyncio.sleep(20)
                        
                        # é‡è©¦ç­‰å¾…
                        try:
                            await page.wait_for_selector("#search", timeout=5000)
                        except:
                            await page.screenshot(path=f"debug_serp_error_{keyword}.png")

                    await asyncio.sleep(3) # é¡å¤–ç­‰å¾… JS æ¸²æŸ“

                    # æŠ“å–æœå°‹çµæœé€£çµ - ä½¿ç”¨æ›´å¯¬é¬†çš„é¸æ“‡å™¨
                    # æ”¹ç‚ºæŠ“å– #search å€åŸŸå…§æ‰€æœ‰å¸¶æœ‰ http çš„é€£çµï¼Œä¸å†ä¾è³´ div.g
                    links = await page.locator("#search a[href^='http']").all()
                    
                    for link in links:
                        href = await link.get_attribute("href")
                        if href and href.startswith("http") and "google.com" not in href:
                            if self.is_valid_d2c_domain(href):
                                # åªä¿ç•™é¦–é æˆ–æ ¹ç¶²åŸŸï¼Œæ–¹ä¾¿å¾ŒçºŒ Sitemap è§£æ
                                parsed = urlparse(href)
                                root_url = f"{parsed.scheme}://{parsed.netloc}"
                                results.add(root_url)

                    # æ¨¡æ“¬çœŸäººç¿»é åœç•™ï¼Œé¿å…è§¸ç™¼é©—è­‰
                    await asyncio.sleep(random.uniform(5, 8))
            
            except Exception as e:
                print(f"âŒ [SERP] æœå°‹å¤±æ•—: {e}")
            finally:
                await browser.close()
        
        print(f"âœ… [SERP] æ‰¾åˆ° {len(results)} å€‹æ½›åœ¨ D2C ç¶²åŸŸ")
        return list(results)

# æ¸¬è©¦ç”¨
if __name__ == "__main__":
    finder = SerpDiscovery()
    domains = asyncio.run(finder.search_google("è‘‰é»ƒç´  æ¨è–¦", pages=10, results_per_page=10))
    print(domains)
