import asyncio
import os
import json
import random
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
import google.generativeai as genai
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # å›åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
load_dotenv(os.path.join(script_dir, '.env'))

class AgentD2CScanner:
    """
    é€šç”¨å‹ D2C æƒæ Agent
    ä¸ä¾è³´ç‰¹å®š CSS Selectorï¼Œè€Œæ˜¯æŠ“å–å…¨é æ–‡å­—å¾Œäº¤ç”± LLM æå–çµæ§‹åŒ–è³‡æ–™ã€‚
    """
    def __init__(self):
        self.api_key = os.environ.get("GOOGLE_API_KEY")
        if not self.api_key:
            print("âš ï¸ [Agent] æœªè¨­å®š GOOGLE_API_KEYï¼ŒAI åˆ†æå°‡å¤±æ•ˆã€‚")
        else:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})

    async def analyze_with_llm(self, html_content, url):
        """å‘¼å« Gemini é€²è¡Œèªç¾©åˆ†æ"""
        if not self.api_key: return {}

        soup = BeautifulSoup(html_content, 'html.parser')
        # ç§»é™¤é›œè¨Š
        for tag in soup(['script', 'style', 'nav', 'footer', 'noscript', 'svg']):
            tag.decompose()
        text = soup.get_text(separator='\n', strip=True)[:15000] # é™åˆ¶é•·åº¦

        prompt = f"""
        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é›»å•†æ•¸æ“šçˆ¬èŸ²ã€‚è«‹åˆ†æä»¥ä¸‹ç”¢å“é é¢çš„ HTML æ–‡å­—å…§å®¹ï¼Œä¸¦æå–çµæ§‹åŒ–è³‡æ–™ã€‚
        
        ç”¢å“ç¶²å€: {url}
        ç¶²é å…§å®¹:
        {text}

        è«‹è¼¸å‡º JSON æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ (è‹¥æ‰¾ä¸åˆ°è«‹å¡« null æˆ– 0):
        - brand: å“ç‰Œåç¨± (å­—ä¸²)
        - title: ç”¢å“å®Œæ•´åç¨± (å­—ä¸²)
        - price: ç›®å‰å”®åƒ¹ (æ•´æ•¸ï¼Œå»é™¤å¹£åˆ¥ç¬¦è™Ÿ)
        - unit_price: å¹³å‡å–®åƒ¹ (æµ®é»æ•¸ï¼Œè‹¥ç„¡æ³•è¨ˆç®—å¡« 0)
        - total_count: ç¸½é¡†æ•¸/åŒ…æ•¸ (æ•´æ•¸ï¼Œè‹¥ç„¡æ³•åˆ¤æ–·å¡« 0)
        - product_highlights: ç”¢å“äº®é» (å­—ä¸²ï¼Œä»¥åˆ†è™Ÿåˆ†éš”ï¼Œæå–å°ˆåˆ©ã€èªè­‰ã€æˆåˆ†å„ªå‹¢ç­‰)
        """

        try:
            response = await self.model.generate_content_async(prompt)
            text = response.text
            
            # æ¸…æ´— Markdown æ¨™è¨˜ (```json ... ```)
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            elif "```" in text:
                text = text.split("```")[1].split("```")[0]
            
            data = json.loads(text.strip())
            
            # å®¹éŒ¯ï¼šè‹¥ AI å›å‚³ Listï¼Œå–ç¬¬ä¸€ç­†
            if isinstance(data, list):
                data = data[0] if data else {}
                
            return data
        except Exception as e:
            print(f"âš ï¸ [Agent] LLM åˆ†æå¤±æ•—: {e}")
            return {}

    async def scan_url(self, url):
        """æƒæå–®ä¸€ URL"""
        print(f"ğŸ¤– [Agent] æ­£åœ¨æƒæ: {url}")
        data = None
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            await stealth_async(page)

            try:
                # éš¨æ©Ÿå»¶é²ï¼Œæ¨¡æ“¬çœŸäºº
                await asyncio.sleep(random.uniform(1, 3))
                
                response = await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                
                # è™•ç† 403/429 é‡è©¦é‚è¼¯ (ç°¡å–®ç‰ˆ)
                if response.status in [403, 429]:
                    print(f"âš ï¸ [Agent] é‡åˆ° {response.status}ï¼Œç­‰å¾… 10 ç§’å¾Œé‡è©¦...")
                    await asyncio.sleep(10)
                    await page.reload()
                
                # [New] ç­‰å¾…åƒ¹æ ¼å…ƒç´ æ¸²æŸ“ (é‡å° Vitabox ç­‰å‹•æ…‹ç¶²ç«™)
                try:
                    # å˜—è©¦ç­‰å¾…å¸¸è¦‹çš„åƒ¹æ ¼ç¬¦è™Ÿæˆ– class
                    await page.wait_for_selector("text=NT$", timeout=3000)
                except:
                    pass # è‹¥æ²’ç­‰åˆ°ä¹Ÿä¸è¦å ±éŒ¯ï¼Œç¹¼çºŒåŸ·è¡Œ
                
                # [New] ç¬¬äºŒé“æ¿¾ç¶² - å‹•æ…‹é©—èº« (Smart Filter)
                # æª¢æŸ¥ og:type æˆ– JSON-LD æ˜¯å¦æ¨™è¨˜ç‚º Productï¼Œé¿å…æµªè²» AI Token åˆ†æéç”¢å“é 
                is_product = await page.evaluate("""() => {
                    const ogType = document.querySelector('meta[property="og:type"]')?.content;
                    const jsonLd = Array.from(document.querySelectorAll('script[type="application/ld+json"]'))
                                        .map(el => el.innerText)
                                        .join('');
                    return ogType === 'product' || jsonLd.includes('"@type": "Product"') || jsonLd.includes('"@type":"Product"');
                }""")
                
                if not is_product:
                    print(f"â© [Agent] è·³ééç”¢å“é é¢ (ç„¡ Product æ¨™è¨˜): {url}")
                    return None

                # æ»¾å‹•é é¢è§¸ç™¼ Lazy Load
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)

                # æŠ“å–åŸºç¤è³‡æ–™ (åœ–ç‰‡èˆ‡ HTML)
                content = await page.content()
                
                # å˜—è©¦æŠ“å– og:image
                image_url = await page.get_attribute("meta[property='og:image']", "content")
                if not image_url:
                    # Fallback: æ‰¾ç¬¬ä¸€å¼µå¤§åœ–
                    imgs = await page.locator("img").all()
                    for img in imgs:
                        src = await img.get_attribute("src")
                        if src and "http" in src and ("jpg" in src or "png" in src):
                            image_url = src
                            break
                
                # LLM åˆ†æ
                ai_data = await self.analyze_with_llm(content, url)
                
                if ai_data:
                    # æ•´åˆè³‡æ–™
                    data = {
                        "source": "D2C_Hunter", # æ¨™è¨˜ä¾†æº
                        "brand": ai_data.get("brand", "Unknown"),
                        "title": ai_data.get("title", "Unknown"),
                        "price": ai_data.get("price", 0),
                        "unit_price": ai_data.get("unit_price", 0),
                        "total_count": ai_data.get("total_count", 0),
                        "url": url,
                        "image_url": image_url or "",
                        "product_highlights": ai_data.get("product_highlights", "")
                    }
                    print(f"âœ… [Agent] æˆåŠŸæå–: {data['title']} (${data['price']})")
                
            except Exception as e:
                print(f"âŒ [Agent] æƒæå¤±æ•— {url}: {e}")
            finally:
                await browser.close()
        
        return data

    async def scan_batch(self, urls):
        """æ‰¹æ¬¡æƒæ"""
        results = []
        # é™åˆ¶ä¸¦ç™¼æ•¸ï¼Œé¿å…è¢«å°é–
        semaphore = asyncio.Semaphore(3) 
        
        async def sem_scan(u):
            async with semaphore:
                return await self.scan_url(u)

        tasks = [sem_scan(u) for u in urls]
        scanned = await asyncio.gather(*tasks)
        
        # éæ¿¾å¤±æ•—çš„çµæœ
        for res in scanned:
            if res: results.append(res)
        return results