import asyncio
import os
import json
import random
import re
import html as html_lib
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from playwright_stealth import stealth_async
try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    from dotenv import load_dotenv
except ImportError:
    def load_dotenv(*args, **kwargs):
        return False

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # å›åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
load_dotenv(os.path.join(script_dir, '.env'))

class AgentD2CScanner:
    """
    é€šç”¨å‹ D2C æƒæ Agent
    ä¸ä¾è³´ç‰¹å®š CSS Selectorï¼Œè€Œæ˜¯æŠ“å–å…¨é æ–‡å­—å¾Œäº¤ç”± LLM æå–çµæ§‹åŒ–è³‡æ–™ã€‚
    """
    def __init__(self):
        self.api_key = os.environ.get("GOOGLE_API_KEY")
        self.llm_timeout_seconds = int(os.environ.get("D2C_LLM_TIMEOUT", "15"))
        self.page_timeout_seconds = 30
        if not self.api_key or genai is None:
            if genai is None:
                print("âš ï¸ [Agent] æœªå®‰è£ google-generativeaiï¼ŒAI åˆ†æå°‡å¤±æ•ˆã€‚")
            else:
                print("âš ï¸ [Agent] æœªè¨­å®š GOOGLE_API_KEYï¼ŒAI åˆ†æå°‡å¤±æ•ˆã€‚")
        else:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})

    @staticmethod
    def _normalize_url(url):
        """å®¹éŒ¯è™•ç†ï¼šæ”¯æ´ç´”ç¶²å€èˆ‡ Markdown æ ¼å¼ `[url](url)`ã€‚"""
        if not isinstance(url, str):
            return ""
        url = url.strip()
        md_match = re.search(r'\((https?://[^\s)]+)\)', url)
        if md_match:
            return md_match.group(1)
        raw_match = re.search(r'https?://[^\s\]]+', url)
        if raw_match:
            return raw_match.group(0)
        return url

    async def _wait_for_price_elements(self, page, url):
        """åœ¨ dump HTML å‰å…ˆç­‰å¾…åƒ¹æ ¼ DOM æ¸²æŸ“ï¼Œæå‡å‹•æ…‹ç«™åƒ¹æ ¼å‘½ä¸­ç‡ã€‚"""
        selector_candidates = [
            ".price",
            ".product-price",
            "div[class*='price']",
            ".price-regular .price",
            ".js-price .price"
        ]

        # Vitabox / Shopline å„ªå…ˆç­‰å¾…è¼ƒç²¾æº–çš„çµ„åˆ
        if "vitabox" in url or "shopline" in url:
            prioritized = ".same-price .price, .price-regular .price, .js-price .price, .product-price, .price"
            try:
                # é¿å…å–®é ç­‰å¾…éä¹…å°è‡´æ•´é«” page timeoutï¼Œè¢«èª¤åˆ¤ç‚ºå¡æ­»
                await page.wait_for_selector(prioritized, state="attached", timeout=8000)
                return
            except:
                pass

        # ä¹äº”ä¹‹ä¸¹ï¼šçŸ­ç­‰å¾…åƒ¹æ ¼å€å¡Šè¼‰å…¥ï¼ˆé¿å…å¤ªæ—©æŠ“åˆ°ã€Œå·²ç†±éŠ·1000ä»½ã€èª¤åˆ¤ç‚ºåƒ¹æ ¼ï¼‰
        if "95dan.com.tw" in (url or ""):
            try:
                await page.wait_for_selector("div.pro_dis_info", state="attached", timeout=5000)
                await page.wait_for_selector("div.pro_dis_info span.price", state="attached", timeout=5000)
            except:
                pass
            return

        # é€šç”¨ fallbackï¼šçŸ­ç­‰å¾…å˜—è©¦ï¼Œä¸å‘½ä¸­å°±ç›´æ¥å¾€ä¸‹ï¼ˆé¿å…å–®é é•·æ™‚é–“å¡ä½ï¼‰
        for selector in selector_candidates:
            try:
                await page.wait_for_selector(selector, state="attached", timeout=1500)
                return
            except PlaywrightTimeoutError:
                continue
            except:
                continue

    async def _extract_price_from_dom(self, page):
        """DOM å„ªå…ˆç­–ç•¥ï¼šå…ˆç›´æ¥æŠ½åƒ¹æ ¼ï¼Œè‹¥æˆåŠŸå¯è¦†è“‹ LLM åƒ¹æ ¼ã€‚"""
        current_url = (page.url or "").lower()

        # ä¹äº”ä¹‹ä¸¹å°ˆç”¨ï¼šå„ªå…ˆè®€å–ç”¢å“åƒ¹æ ¼å€å¡Š
        # <div class="pro_dis_info"><span class="old-price">NT$400</span><span class="price">NT$350</span></div>
        if "95dan.com.tw" in current_url:
            try:
                exact_price_text = await page.evaluate("""() => {
                    const node = document.querySelector('div.pro_dis_info span.price');
                    return node ? node.textContent : '';
                }""")
                exact_price = int(re.sub(r'[^\d]', '', exact_price_text or '') or 0)
                if 100 <= exact_price <= 200000:
                    return exact_price
            except:
                pass

            # fallbackï¼šè‹¥ span.price æŠ“ä¸åˆ°ï¼Œå†å˜—è©¦åœ¨ pro_dis_info å€å¡Šä¸­æŠ½æœ€å¾Œä¸€å€‹é‡‘é¡
            try:
                block_text = await page.evaluate("""() => {
                    const node = document.querySelector('div.pro_dis_info');
                    return node ? node.textContent : '';
                }""") or ""
                nums = re.findall(r'\d{2,6}', block_text.replace(',', ''))
                if nums:
                    # é€šå¸¸æœ€å¾Œä¸€å€‹æ˜¯ sale priceï¼Œå‰ä¸€å€‹æ˜¯ old-price
                    v = int(nums[-1])
                    if 100 <= v <= 200000:
                        return v
            except:
                pass

            # ä¹äº”ä¹‹ä¸¹è‹¥æœªå‘½ä¸­æ˜ç¢ºåƒ¹æ ¼ï¼Œç›´æ¥å›å‚³ 0ï¼›äº¤ç”± HTML/JSON-LD åƒ¹æ ¼ä¾†æºè™•ç†
            return 0

        selectors = [
            ".same-price .price",
            ".same-price .price-regular .price",
            ".price-regular .price",
            ".js-price .price",
            ".price-sale .price",
            ".product-price",
            ".special-price",
            "div[class*='price']",
            "span.price",
            "div.price",
            ".price"
        ]

        for selector in selectors:
            try:
                locator = page.locator(selector)
                count = await locator.count()
                if count == 0:
                    continue

                # åªæª¢æŸ¥å‰å¹¾å€‹å…ƒç´ ï¼Œé¿å…æŠ“å¤ªæ…¢
                check_n = min(count, 8)
                for i in range(check_n):
                    el = locator.nth(i)
                    if not await el.is_visible():
                        continue
                    p_text = (await el.text_content() or "").strip()
                    if not any(c.isdigit() for c in p_text):
                        continue
                    p_val = int(re.sub(r'[^\d]', '', p_text) or 0)
                    # åˆç†åƒ¹æ ¼å€é–“ï¼Œé¿å…èª¤æŠ“è©•åˆ†/ä»¶æ•¸
                    if 100 <= p_val <= 200000:
                        return p_val
            except:
                continue

        # Fallbackï¼šå…¨æ–‡æ‰¾ NT$ / TWD / $
        try:
            body_text = await page.locator("body").text_content() or ""
            matches = re.findall(r'(?:NT\$|TWD\s*|\$)\s*(\d{1,3}(?:,\d{3})+|\d{3,6})', body_text)
            for m in matches:
                val = int(m.replace(',', ''))
                if 100 <= val <= 200000:
                    return val
        except:
            pass

        return 0

    @staticmethod
    def _looks_like_product_url(url):
        """URL å±¤ç´šçš„ç”¢å“åˆ¤æ–·ï¼Œé¿å…éƒ¨åˆ†ç«™å°ç¼ºå°‘ og:type æ™‚è¢«èª¤åˆ¤ã€‚"""
        u = (url or "").lower()
        product_tokens = ["/product", "/products", "/shop/", "lutein", "fish-oil", "probiotic"]
        return any(t in u for t in product_tokens)

    async def analyze_with_llm(self, html_content, url):
        """å‘¼å« Gemini é€²è¡Œèªç¾©åˆ†æ"""
        if not self.api_key or genai is None:
            return {}

        soup = BeautifulSoup(html_content, 'html.parser')
        # ç§»é™¤é›œè¨Š
        for tag in soup(['script', 'style', 'nav', 'footer', 'noscript', 'svg']):
            tag.decompose()
        text = soup.get_text(separator='\n', strip=True)[:15000] # é™åˆ¶é•·åº¦

        prompt = f"""
        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é›»å•†æ•¸æ“šçˆ¬èŸ²ã€‚è«‹åˆ†æä»¥ä¸‹ç”¢å“é é¢çš„ HTML æ–‡å­—å…§å®¹ï¼Œä¸¦æå–çµæ§‹åŒ–è³‡æ–™ã€‚
        
        ç”¢å“ç¶²å€: {url}
        ç¶²é å…§å®¹:
        {text}

        è«‹è¼¸å‡º JSON æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ (è‹¥æ‰¾ä¸åˆ°è«‹å¡« null æˆ– 0):
        - brand: å“ç‰Œåç¨± (å­—ä¸²)
        - title: ç”¢å“å®Œæ•´åç¨± (å­—ä¸²)
        - price: ç›®å‰å”®åƒ¹ (æ•´æ•¸ï¼Œå»é™¤å¹£åˆ¥ç¬¦è™Ÿ)
        - unit_price: å¹³å‡å–®åƒ¹ (æµ®é»æ•¸ï¼Œè‹¥ç„¡æ³•è¨ˆç®—å¡« 0)
        - total_count: ç¸½é¡†æ•¸/åŒ…æ•¸ (æ•´æ•¸ï¼Œè‹¥ç„¡æ³•åˆ¤æ–·å¡« 0)
        - product_highlights: ç”¢å“äº®é» (å­—ä¸²ï¼Œä»¥åˆ†è™Ÿåˆ†éš”ï¼Œæå–å°ˆåˆ©ã€èªè­‰ã€æˆåˆ†å„ªå‹¢ç­‰)
        """

        try:
            response = await asyncio.wait_for(
                self.model.generate_content_async(prompt),
                timeout=self.llm_timeout_seconds
            )
            text = response.text
            
            # æ¸…æ´— Markdown æ¨™è¨˜ (```json ... ```)
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            elif "```" in text:
                text = text.split("```")[1].split("```")[0]
            
            data = json.loads(text.strip())
            
            # å®¹éŒ¯ï¼šè‹¥ AI å›å‚³ Listï¼Œå–ç¬¬ä¸€ç­†
            if isinstance(data, list):
                data = data[0] if data else {}
                
            return data
        except asyncio.TimeoutError:
            print(f"âš ï¸ [Agent] LLM é€¾æ™‚ï¼ˆ>{self.llm_timeout_seconds}sï¼‰ï¼Œæ”¹ç”¨é LLM fallback")
            return {}
        except Exception as e:
            print(f"âš ï¸ [Agent] LLM åˆ†æå¤±æ•—: {e}")
            return {}

    def _extract_basic_info_from_html(self, html_content, url):
        """LLM å¤±æ•—æ™‚çš„æœ€å°å¯ç”¨è³‡æ–™ã€‚"""
        title = "Unknown"
        brand = "Unknown"

        try:
            soup = BeautifulSoup(html_content or "", 'html.parser')
            h1 = soup.select_one('h1')
            og_title = soup.select_one('meta[property="og:title"]')
            doc_title = soup.title.string.strip() if soup.title and soup.title.string else ""

            title = (
                (h1.get_text(strip=True) if h1 else "")
                or (og_title.get('content', '').strip() if og_title else "")
                or doc_title
                or "Unknown"
            )

            if "vitabox" in (url or "").lower() or "vitabox" in title.lower():
                brand = "Vitabox"
        except:
            pass

        return {"brand": brand, "title": title}

    def _extract_95dan_highlights_and_count(self, html_content):
        """ä¹äº”ä¹‹ä¸¹é é¢å°ˆç”¨ï¼šæå–å•†å“ç‰¹è‰²èˆ‡å–®åŒ…è£æ•¸é‡ï¼ˆç²’/åŒ…ï¼‰ã€‚"""
        highlights = ""
        total_count = 0

        if not html_content:
            return {"product_highlights": highlights, "total_count": total_count}

        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # å•†å“ç‰¹è‰²ï¼š.pro_info_div ä¸­ title ç‚ºã€Œå•†å“ç‰¹è‰²ã€çš„ ul/li
            for block in soup.select("div.pro_info_div"):
                title_node = block.select_one("div.pro_info_title")
                title_text = title_node.get_text(strip=True) if title_node else ""

                if "å•†å“ç‰¹è‰²" in title_text:
                    items = [li.get_text(" ", strip=True) for li in block.select("ul.pro_info_ul li")]
                    items = [x for x in items if x]
                    if items:
                        highlights = ";".join(items)

                if "å•†å“è³‡è¨Š" in title_text:
                    info_text = block.get_text(" ", strip=True)
                    # ä¾‹ï¼šè¦æ ¼ï¼š30ç²’/åŒ…ï¼Œ15å¤©ä»½
                    m = re.search(r"è¦æ ¼\s*[:ï¼š]\s*(\d+)\s*(?:ç²’|é¡†|éŒ |åŒ…)\s*/\s*åŒ…", info_text)
                    if m:
                        total_count = int(m.group(1))

            # fallbackï¼šè‹¥å•†å“è³‡è¨Šå€å¡Šæ²’æŠ“åˆ°ï¼Œå˜—è©¦å…¨é è¦æ ¼æ–‡å­—
            if total_count == 0:
                full_text = soup.get_text(" ", strip=True)
                m = re.search(r"è¦æ ¼\s*[:ï¼š]\s*(\d+)\s*(?:ç²’|é¡†|éŒ |åŒ…)\s*/\s*åŒ…", full_text)
                if m:
                    total_count = int(m.group(1))
        except:
            pass

        return {"product_highlights": highlights, "total_count": total_count}

    def _extract_price_from_html_content(self, html_content):
        """
        ç¬¬äºŒè¼ªåƒ¹æ ¼ç­–ç•¥ï¼šç›´æ¥å¾ HTML / script è³‡æ–™å±¤æå–åƒ¹æ ¼ã€‚
        å„ªå…ˆé †åºï¼š
        1) JSON-LD Offer price
        2) app.value('product', JSON.parse('...')) ä¸­çš„ price/price_sale/variations
        """
        if not html_content:
            return 0

        # 0) ä¹äº”ä¹‹ä¸¹ HTML å€å¡Šç›´æŠ“ï¼š
        # <div class="pro_dis_info"><span class="old-price">NT$400</span> <span class="price">NT$350</span></div>
        try:
            block_match = re.search(r'<div[^>]*class="[^"]*pro_dis_info[^"]*"[^>]*>(.*?)</div>', html_content, re.IGNORECASE | re.DOTALL)
            if block_match:
                block = block_match.group(1)
                sale_match = re.search(r'<span[^>]*class="[^"]*price[^"]*"[^>]*>\s*NT\$?\s*([\d,]+)', block, re.IGNORECASE)
                if sale_match:
                    val = int(sale_match.group(1).replace(',', ''))
                    if val > 0:
                        return val
                nums = re.findall(r'NT\$?\s*([\d,]+)', block, re.IGNORECASE)
                if nums:
                    val = int(nums[-1].replace(',', ''))
                    if val > 0:
                        return val
        except:
            pass

        # 1) JSON-LD
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            for tag in soup.select("script[type='application/ld+json']"):
                raw = (tag.string or tag.text or "").strip()
                if not raw:
                    continue
                try:
                    data = json.loads(raw)
                except:
                    continue
                payloads = data if isinstance(data, list) else [data]
                for node in payloads:
                    if not isinstance(node, dict):
                        continue
                    offers = node.get("offers")
                    if isinstance(offers, dict):
                        p = offers.get("price")
                        if isinstance(p, (int, float)) and p > 0:
                            return int(round(p))
                        if isinstance(p, str):
                            val = int(re.sub(r'[^\d]', '', p) or 0)
                            if val > 0:
                                return val
        except:
            pass

        # 2) Shopline product data from app.value('product', JSON.parse('...'))
        try:
            m = re.search(r"app\.value\('product',\s*JSON\.parse\('(.+?)'\)\);", html_content, re.DOTALL)
            if m:
                payload = m.group(1)
                payload = payload.encode('utf-8').decode('unicode_escape')
                payload = html_lib.unescape(payload)
                product = json.loads(payload)

                candidates = []

                # ä¸»åƒ¹
                for key in ["price_sale", "price", "lowest_member_price"]:
                    obj = product.get(key) or {}
                    cents = obj.get("cents", 0)
                    if isinstance(cents, (int, float)) and cents > 0:
                        candidates.append(int(cents))

                # variations åƒ¹æ ¼ï¼ˆå–æœ€å°æ­£å€¼ï¼Œé€šå¸¸æ˜¯é¡¯ç¤ºåƒ¹ï¼‰
                for v in product.get("variations", []) or []:
                    if not isinstance(v, dict):
                        continue
                    for key in ["price_sale", "price", "member_price"]:
                        obj = v.get(key) or {}
                        cents = obj.get("cents", 0)
                        if isinstance(cents, (int, float)) and cents > 0:
                            candidates.append(int(cents))

                if candidates:
                    return min(candidates)
        except Exception as e:
            print(f"âš ï¸ [Agent] HTML åƒ¹æ ¼è§£æå¤±æ•—: {e}")

        return 0

    def _extract_shopline_price_legacy(self, html_content):
        """
        Legacy Vitabox/Shopline regex strategy transplant:
        app.value('product', {...});
        """
        if not html_content:
            return 0

        # 1) Legacy pattern: app.value('product', { ... });
        try:
            m = re.search(r"app\.value\('product',\s*(\{.*?\})\s*\);", html_content, re.DOTALL)
            if m:
                obj = json.loads(m.group(1))
                price = obj.get("price")
                if isinstance(price, dict):
                    cents = price.get("cents", 0)
                    if isinstance(cents, (int, float)) and cents > 0:
                        return int(cents)
                if isinstance(price, (int, float)) and price > 0:
                    return int(price)
                if isinstance(price, str):
                    v = int(re.sub(r'[^\d]', '', price) or 0)
                    if v > 0:
                        return v
        except Exception as e:
            print(f"âš ï¸ [Agent] Legacy Shopline ç‰©ä»¶åƒ¹æ ¼è§£æå¤±æ•—: {e}")

        # 2) Backward compatible pattern currently used in scanner
        try:
            m = re.search(r"app\.value\('product',\s*JSON\.parse\('(.+?)'\)\);", html_content, re.DOTALL)
            if m:
                payload = m.group(1)
                payload = payload.encode('utf-8').decode('unicode_escape')
                payload = html_lib.unescape(payload)
                product = json.loads(payload)

                candidates = []
                for key in ["price_sale", "price", "lowest_member_price"]:
                    obj = product.get(key) or {}
                    cents = obj.get("cents", 0)
                    if isinstance(cents, (int, float)) and cents > 0:
                        candidates.append(int(cents))

                for v in product.get("variations", []) or []:
                    if not isinstance(v, dict):
                        continue
                    for key in ["price_sale", "price", "member_price"]:
                        obj = v.get(key) or {}
                        cents = obj.get("cents", 0)
                        if isinstance(cents, (int, float)) and cents > 0:
                            candidates.append(int(cents))

                if candidates:
                    return min(candidates)
        except Exception as e:
            print(f"âš ï¸ [Agent] Legacy Shopline JSON.parse åƒ¹æ ¼è§£æå¤±æ•—: {e}")

        return 0

    async def scan_url(self, url):
        """æƒæå–®ä¸€ URL"""
        url = self._normalize_url(url)
        if not url:
            print("âŒ [Agent] ç„¡æ•ˆ URLï¼Œè·³é")
            return None
        print(f"[INFO] Start scraping: {url}...")
        print(f"ğŸ¤– [Agent] æ­£åœ¨æƒæ: {url}")
        data = None
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            page.set_default_timeout(30000)
            page.set_default_navigation_timeout(30000)
            await stealth_async(page)

            async def _run_page_work():
                nonlocal data
                # éš¨æ©Ÿå»¶é²ï¼Œæ¨¡æ“¬çœŸäºº
                await asyncio.sleep(random.uniform(1, 3))
                
                response = await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                
                # è™•ç† 403/429 é‡è©¦é‚è¼¯ (ç°¡å–®ç‰ˆ)
                if response.status in [403, 429]:
                    print(f"âš ï¸ [Agent] é‡åˆ° {response.status}ï¼Œç­‰å¾… 10 ç§’å¾Œé‡è©¦...")
                    await asyncio.sleep(10)
                    await page.reload(wait_until="domcontentloaded", timeout=30000)
                
                # ç­‰å¾…åƒ¹æ ¼å…ƒç´ æ¸²æŸ“ (åœ¨ dump HTML å‰åŸ·è¡Œ)
                await self._wait_for_price_elements(page, url)
                
                # å…ˆæŠ“ DOM åƒ¹æ ¼ï¼Œä¾›å¾ŒçºŒç”¢å“é åˆ¤æ–·èˆ‡åƒ¹æ ¼è¦†è“‹
                dom_price = await self._extract_price_from_dom(page)

                # ç¬¬äºŒé“æ¿¾ç¶² - å‹•æ…‹é©—èº« (Smart Filter)
                # æª¢æŸ¥ og:type æˆ– JSON-LD æ˜¯å¦æ¨™è¨˜ç‚º Productï¼Œé¿å…æµªè²» AI Token åˆ†æéç”¢å“é 
                is_product_meta = await page.evaluate("""() => {
                    const ogType = document.querySelector('meta[property="og:type"]')?.content;
                    const jsonLd = Array.from(document.querySelectorAll('script[type="application/ld+json"]'))
                                        .map(el => el.innerText)
                                        .join('');
                    return ogType === 'product' || jsonLd.includes('"@type": "Product"') || jsonLd.includes('"@type":"Product"');
                }""")
                is_product = is_product_meta or self._looks_like_product_url(url) or dom_price > 0
                
                if not is_product:
                    print(f"â© [Agent] è·³ééç”¢å“é é¢ (ç„¡ Product æ¨™è¨˜): {url}")
                    return None

                # æ»¾å‹•é é¢è§¸ç™¼ Lazy Load
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)

                # æŠ“å–åŸºç¤è³‡æ–™ (åœ–ç‰‡èˆ‡ HTML)
                content = await page.content()
                html_price = self._extract_price_from_html_content(content)
                legacy_shopline_price = 0
                if "vitabox" in url or "shopline" in url:
                    legacy_shopline_price = self._extract_shopline_price_legacy(content)
                if html_price == 0 and dom_price == 0 and ("vitabox" in url or "shopline" in url):
                    try:
                        with open("debug_vitabox_page.html", "w", encoding="utf-8") as f:
                            f.write(content)
                    except Exception as e:
                        print(f"âš ï¸ [Agent] ç„¡æ³•å¯«å…¥ Vitabox debug HTML: {e}")
                
                # å˜—è©¦æŠ“å– og:image
                image_url = await page.get_attribute("meta[property='og:image']", "content")
                if not image_url:
                    # Fallback: æ‰¾ç¬¬ä¸€å¼µå¤§åœ–
                    imgs = await page.locator("img").all()
                    for img in imgs:
                        src = await img.get_attribute("src")
                        if src and "http" in src and ("jpg" in src or "png" in src):
                            image_url = src
                            break
                
                # LLM åˆ†æï¼ˆä¹äº”ä¹‹ä¸¹å…ˆèµ°è¦å‰‡å¼•æ“ï¼Œé¿å… API å»¶é²é€ æˆæ•´é«” timeoutï¼‰
                if "95dan.com.tw" in (url or ""):
                    ai_data = {}
                else:
                    ai_data = await self.analyze_with_llm(content, url)
                basic_data = self._extract_basic_info_from_html(content, url)
                d95_meta = self._extract_95dan_highlights_and_count(content) if "95dan.com.tw" in (url or "") else {}

                # æ•´åˆè³‡æ–™ï¼ˆLLM æˆåŠŸ/å¤±æ•—éƒ½æœƒçµ„è£çµæœï¼Œé¿å… pendingï¼‰
                final_price = (ai_data or {}).get("price", 0)
                # DOM / HTML script å„ªå…ˆç­–ç•¥
                # ä¹äº”ä¹‹ä¸¹å…ˆä¿¡ä»» HTML/JSON-LDï¼ˆé¿å… DOM æŠ“åˆ°ã€Œå·²ç†±éŠ·1000ä»½ã€ï¼‰
                if "95dan.com.tw" in (url or ""):
                    if html_price > 0:
                        final_price = html_price
                    elif dom_price > 0:
                        final_price = dom_price
                else:
                    if dom_price > 0:
                        final_price = dom_price
                    elif legacy_shopline_price > 0:
                        final_price = legacy_shopline_price
                    elif html_price > 0:
                        final_price = html_price

                data = {
                    "source": "D2C_Hunter", # æ¨™è¨˜ä¾†æº
                    "brand": (ai_data or {}).get("brand") or basic_data.get("brand", "Unknown"),
                    "title": (ai_data or {}).get("title") or basic_data.get("title", "Unknown"),
                    "price": int(final_price or 0),
                    "unit_price": (ai_data or {}).get("unit_price", 0),
                    "total_count": (ai_data or {}).get("total_count", 0) or d95_meta.get("total_count", 0),
                    "url": url,
                    "image_url": image_url or "",
                    "product_highlights": (ai_data or {}).get("product_highlights", "") or d95_meta.get("product_highlights", "")
                }
                print(f"âœ… [Agent] æˆåŠŸæå–: {data['title']} (${data['price']})")
                
            try:
                await asyncio.wait_for(_run_page_work(), timeout=self.page_timeout_seconds)
            except (PlaywrightTimeoutError, asyncio.TimeoutError):
                print(f"[WARN] Timeout skipping: {url}")
                return None
            except Exception as e:
                print(f"âŒ [Agent] æƒæå¤±æ•— {url}: {e}")
            finally:
                await browser.close()
        
        return data

    async def scan_batch(self, urls):
        """æ‰¹æ¬¡æƒæ"""
        results = []
        # é™åˆ¶ä¸¦ç™¼æ•¸ï¼Œé¿å…è¢«å°é–
        semaphore = asyncio.Semaphore(3)

        async def sem_scan(u):
            async with semaphore:
                return await self.scan_url(u)

        tasks = [sem_scan(u) for u in urls]
        scanned = await asyncio.gather(*tasks)

        # éæ¿¾å¤±æ•—çš„çµæœ
        for res in scanned:
            if res:
                results.append(res)
        return results


class D2CScanner:
    """å‘å¾Œç›¸å®¹å°è£ï¼šæä¾›åŒæ­¥ä»‹é¢ï¼Œæ–¹ä¾¿è…³æœ¬ç›´æ¥å‘¼å«ã€‚"""
    def __init__(self):
        self._scanner = AgentD2CScanner()

    def scan_url(self, url):
        return asyncio.run(self._scanner.scan_url(url))