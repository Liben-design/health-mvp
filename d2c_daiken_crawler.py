import asyncio
import random
import pandas as pd
import os
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async

async def random_sleep(min_sec=2, max_sec=5):
    """ç•°æ­¥ç­‰å¾…ä¸€å€‹éš¨æ©Ÿçš„ç§’æ•¸ï¼Œæ¨¡æ“¬çœŸäººåœé “ã€‚"""
    sleep_time = random.uniform(min_sec, max_sec)
    # print(f"Simulating human behavior: waiting for {sleep_time:.2f} seconds...")
    await asyncio.sleep(sleep_time)

def calculate_unit_price(title, price, description=""):
    """å¾æ¨™é¡Œè¨ˆç®—ç¸½é¡†ç²’æ•¸èˆ‡å–®ä½åƒ¹æ ¼"""
    if not isinstance(title, str) or not price: return None, 0
    unit_count, bundle_size = None, 1
    
    # 1. å°‹æ‰¾å–®å“æ•¸é‡ (å„ªå…ˆæŸ¥æ¨™é¡Œï¼Œè‹¥ç„¡å‰‡æŸ¥æè¿°)
    # æ’é™¤ "30åŒ…å…¥" é€™ç¨®å¯«æ³•é€ æˆçš„èª¤åˆ¤ï¼Œå…ˆæ‰¾ç´”æ•¸é‡è©
    count_regex = r'(\d+)\s*[ç²’é¡†éŒ åŒ…]'
    match = re.search(count_regex, title)
    if match: 
        unit_count = int(match.group(1))
    
    # è‹¥æ¨™é¡Œæ²’æ‰¾åˆ°ï¼Œå˜—è©¦å¾æè¿°ä¸­æ‰¾ï¼Œä½†å„ªå…ˆå°‹æ‰¾æ˜ç¢ºçš„ "å…§å®¹é‡/è¦æ ¼" æ¨™ç¤º
    if not unit_count and description:
        spec_match = re.search(r'(?:å…§å®¹é‡|è¦æ ¼)[ï¼š:]\s*(\d+)\s*[ç²’é¡†éŒ åŒ…]', description)
        if spec_match:
            unit_count = int(spec_match.group(1))
        else:
            # æœ€å¾Œæ‰‹æ®µï¼šæœå°‹æè¿°ä¸­å‡ºç¾çš„ç¬¬ä¸€å€‹æ•¸é‡ (é¢¨éšªè¼ƒé«˜ï¼Œä½†å› ç‚ºå·²éæ¿¾é›œè¨Šå€å¡Šï¼Œç›¸å°å®‰å…¨)
            match = re.search(count_regex, description)
            if match: unit_count = int(match.group(1))

    # 2. å°‹æ‰¾çµ„æ•¸ (Bundle Size)
    # ä½¿ç”¨æ›´åš´æ ¼çš„ Regex é¿å…åŒ¹é…åˆ° "30åŒ…" ä¸­çš„ 30
    # åŒ¹é… x3, *3, 3å…¥ (é€šå¸¸çµ„æ•¸ä¸æœƒå¤ªå¤§ï¼Œé™åˆ¶ 1-2 ä½æ•¸ä»¥é˜²èª¤åˆ¤)
    bundle_match = re.search(r'[xX*]\s*(\d{1,2})\b', title)
    if bundle_match:
        bundle_size = int(bundle_match.group(1))
    else:
        # åŒ¹é… " 3å…¥", " 3ä»¶çµ„", "(3å…¥)" (éœ€ç¢ºèªå‰é¢æœ‰ç©ºæ ¼ã€æ‹¬è™Ÿæˆ–æ¨™é»)
        bundle_match = re.search(r'[\s\uff0c\(\uff08](\d{1,2})\s*[å…¥ä»¶çµ„]', title)
        if bundle_match: bundle_size = int(bundle_match.group(1))
    
    # é˜²å‘†ï¼šå¦‚æœçµ„æ•¸å¤§æ–¼ 10 ä¸”èˆ‡å–®å“æ•¸é‡ç›¸åŒï¼Œæ¥µå¯èƒ½æ˜¯èª¤åˆ¤ (ä¾‹å¦‚ "30åŒ…å…¥" è¢«èª¤åˆ¤ç‚º count=30, bundle=30)
    if unit_count and bundle_size > 10 and unit_count == bundle_size:
        bundle_size = 1
        
    if unit_count:
        total_count = unit_count * bundle_size
        return total_count, round(price / total_count, 2)
    return None, 0

def extract_tags(text):
    """å¾æ–‡æœ¬ä¸­æå–ç”¢å“æ¨™ç±¤"""
    tags = []
    if not isinstance(text, str): return ""
    
    # è‘‰é»ƒç´ 
    if re.search(r"æ¸¸é›¢å‹|Free form", text, re.IGNORECASE): tags.append("âœ…æ¸¸é›¢å‹")
    if re.search(r"FloraGLO", text, re.IGNORECASE): tags.append("ğŸ’FloraGLO")
    if re.search(r"10[:ï¼š]2", text): tags.append("âš–ï¸10:2æ¯”ä¾‹")
    
    # é­šæ²¹
    if re.search(r"Omega-?3", text, re.IGNORECASE): tags.append("ğŸŸOmega-3")
    if re.search(r"rTG", text, re.IGNORECASE): tags.append("ğŸ§¬rTGå‹")
    if re.search(r"IFOS", text, re.IGNORECASE): tags.append("ğŸ†IFOSèªè­‰")
    if re.search(r"80%|84%", text): tags.append("ğŸ“ˆé«˜æ¿ƒåº¦")

    # ç›Šç”ŸèŒ/é…µç´ /å…¶ä»–
    if re.search(r"ç›Šç”ŸèŒ|ä¹³é…¸èŒ", text): tags.append("ğŸ¦ ç›Šç”ŸèŒ")
    if re.search(r"300å„„", text): tags.append("ğŸ”¢300å„„")
    if re.search(r"UC-?II|UC2", text, re.IGNORECASE): tags.append("ğŸ¦´UC-II")
    if re.search(r"ç‘ªå¡|Maca", text, re.IGNORECASE): tags.append("ğŸ’ªç‘ªå¡")
    if re.search(r"Q10", text, re.IGNORECASE): tags.append("âš¡Q10")

    # é€šç”¨èªè­‰
    if re.search(r"SNQ", text, re.IGNORECASE): tags.append("ğŸ…SNQèªè­‰")
    if re.search(r"SGS", text, re.IGNORECASE): tags.append("ğŸ›¡ï¸SGSæª¢é©—")
    if re.search(r"Monde Selection", text, re.IGNORECASE): tags.append("ğŸ¥‡ä¸–ç•Œé‡‘ç")
    
    return " ".join(tags) if tags else ""

async def scrape_daiken_all_products():
    """
    æ‰¹é‡æŠ“å–å¤§ç ”ç”Ÿé†«æ‰€æœ‰ç”¢å“è³‡æ–™ã€‚
    1. è¨ªå•å…¨éƒ¨å•†å“é é¢å–å¾—é€£çµã€‚
    2. éæ­·é€£çµï¼Œä½¿ç”¨éš±èº«æ¨¡å¼èˆ‡ og:image ç­–ç•¥æŠ“å–è©³æƒ…ã€‚
    """
    list_url = "https://www.daikenshop.com/allgoods.php"
    base_url = "https://www.daikenshop.com"
    all_data = []
    
    # é–‹å•Ÿ Headless æ¨¡å¼ä»¥åŠ å¿«æ‰¹é‡è™•ç†é€Ÿåº¦ï¼Œä¸¦æ¸›å°‘å¹²æ“¾
    headless_mode = True 

    async with async_playwright() as p:
        print(f"å•Ÿå‹•ç€è¦½å™¨ (Headless: {headless_mode})...")
        browser = await p.chromium.launch(headless=headless_mode)
        
        # --- æ­¥é©Ÿ 1: å–å¾—æ‰€æœ‰ç”¢å“é€£çµ ---
        # å…ˆå»ºç«‹ä¸€å€‹åˆå§‹ Context ç”¨æ–¼æŠ“å–åˆ—è¡¨
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        await stealth_async(page) # å•Ÿç”¨éš±èº«

        print(f"æ­£åœ¨å‰å¾€å…¨éƒ¨å•†å“é é¢: {list_url}")
        await page.goto(list_url, wait_until='networkidle', timeout=60000)
        await random_sleep(2, 3)

        # è™•ç† Cookie (åˆ—è¡¨é ä¹Ÿå¯èƒ½æœ‰)
        try:
            if await page.locator('text="åŒæ„"').count() > 0:
                await page.locator('text="åŒæ„"').first.click()
                print("å·²æ¥å— Cookieã€‚")
        except:
            pass

        # æ»¾å‹•é é¢ç¢ºä¿è¼‰å…¥æ‰€æœ‰å•†å“
        print("æ­£åœ¨æ»¾å‹•é é¢ä»¥è¼‰å…¥åˆ—è¡¨...")
        for _ in range(3):
            await page.evaluate('window.scrollBy(0, window.innerHeight)')
            await asyncio.sleep(1)

        # è§£æé€£çµ
        content = await page.content()
        soup = BeautifulSoup(content, 'html.parser')
        
        product_links = set()
        # æŠ“å–æ‰€æœ‰å«æœ‰ product.php?code= çš„é€£çµ
        for a in soup.find_all('a', href=True):
            href = a['href']
            if 'product.php?code=' in href:
                full_link = urljoin(base_url, href)
                product_links.add(full_link)
        
        links = list(product_links)
        print(f"å…±ç™¼ç¾ {len(links)} å€‹ä¸é‡è¤‡çš„ç”¢å“é€£çµã€‚")
        
        # é—œé–‰åˆ—è¡¨é çš„ Contextï¼Œæº–å‚™é€²å…¥æ‰¹é‡æŠ“å–
        await context.close()
        context = None
        page = None

        # --- æ­¥é©Ÿ 2: æ‰¹é‡æŠ“å–è©³æƒ… ---
        for i, link in enumerate(links):
            retries = 0
            max_retries = 2
            success = False

            while retries <= max_retries and not success:
                # æ¯ 30 ç­†è«‹æ±‚é é˜²æ€§é‡ç½®ä¸€æ¬¡ Contextï¼Œæˆ–è€…å¦‚æœå‰›æ‰å¤±æ•—äº†(contextè¢«è¨­ç‚ºNone)
                if context is None or (i > 0 and i % 30 == 0 and retries == 0):
                    if context:
                        print(f"\n--- å·²è™•ç† {i} ç­†è³‡æ–™ï¼Œå•Ÿå‹•é é˜²æ€§å†·å»èˆ‡ç’°å¢ƒé‡ç½® ---")
                        print("å†·å» 20 ç§’...")
                        await asyncio.sleep(20)
                        await context.close()
                        print("èˆŠç€è¦½å™¨ç’°å¢ƒå·²é—œé–‰ã€‚")
                    
                    print("æ­£åœ¨å»ºç«‹æ–°çš„ç€è¦½å™¨ç’°å¢ƒï¼ˆæ›´æ›èº«ä»½ï¼‰...")
                    context = await browser.new_context(
                        viewport={'width': 1920, 'height': 1080},
                        user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
                    )
                    page = await context.new_page()
                    await stealth_async(page)

                if retries > 0:
                    print(f"\n[{i+1}/{len(links)}] æ­£åœ¨é‡è©¦: {link} (ç¬¬ {retries} æ¬¡é‡è©¦)")
                else:
                    print(f"\n[{i+1}/{len(links)}] æ­£åœ¨è™•ç†: {link}")
            
                try:
                    # å‰å¾€ç”¢å“é 
                    await page.goto(link, wait_until='networkidle', timeout=60000)
                    await random_sleep(4, 7) # å†æ¬¡ç¨å¾®å¢åŠ ç­‰å¾…æ™‚é–“

                    # å†æ¬¡æª¢æŸ¥ Cookie (æœ‰æ™‚æ›é æœƒé‡è·³)
                    try:
                        if await page.locator('text="åŒæ„"').count() > 0:
                            await page.locator('text="åŒæ„"').first.click(timeout=2000)
                    except:
                        pass

                    # ç­‰å¾…é—œéµå…ƒç´  (åƒ¹æ ¼)ï¼Œç¢ºä¿é é¢è¼‰å…¥å®Œæˆ
                    try:
                        await page.locator('text="å»ºè­°å”®åƒ¹"').first.wait_for(state='visible', timeout=10000)
                    except:
                        print("ç­‰å¾…åƒ¹æ ¼è¶…æ™‚ï¼Œå˜—è©¦ç›´æ¥è§£æ...")

                    # è§£æå…§å®¹
                    content = await page.content()
                    soup = BeautifulSoup(content, 'html.parser')

                    # ç”¢å“åç¨±
                    h1 = soup.find('h1')
                    name = h1.get_text(strip=True) if h1 else "Unknown"

                    # æª¢æŸ¥æ˜¯å¦è¢«å°é– (403)
                    if "403" in name or "Forbidden" in name or "Access Denied" in name:
                        raise Exception(f"åµæ¸¬åˆ°å°é–é é¢ (Title: {name})")

                    # åƒ¹æ ¼
                    original_price_tag = soup.find(string=re.compile("å»ºè­°å”®åƒ¹"))
                    original_price_text = original_price_tag.parent.get_text() if original_price_tag else ""
                    
                    special_price_tag = soup.find(string=re.compile("å„ªæƒ åƒ¹"))
                    special_price_text = special_price_tag.parent.get_text() if special_price_tag else "0"

                    op_match = re.search(r'\d[\d,]*', original_price_text)
                    original_price_val = int(op_match.group().replace(',', '')) if op_match else 0
                    
                    sp_match = re.search(r'\d[\d,]*', special_price_text)
                    special_price_val = int(sp_match.group().replace(',', '')) if sp_match else 0

                    # åœ–ç‰‡ (å„ªå…ˆä½¿ç”¨ og:image ç­–ç•¥)
                    image_url = ""
                    og_img = soup.find("meta", property="og:image")
                    if og_img and og_img.get("content"):
                        image_url = og_img["content"]
                    
                    # --- æ–°å¢ï¼šæŠ“å–è¦æ ¼ ---
                    # æŠ“å–æ•´å€‹æè¿°å€å¡Šç”¨æ–¼åˆ†æ
                    # æ”¹é€²ï¼šåªæŠ“å–ç”¢å“æè¿°ç›¸é—œçš„å€å¡Šï¼Œé¿å…æŠ“åˆ°é é¦–é å°¾çš„ "9æŠ˜", "5åŒ…" ç­‰é›œè¨Š
                    desc_text = ""
                    content_selectors = [".product-description", ".detail_content", ".product_detail_content", "div.editor", ".product-intro", ".product-info-main"]
                    
                    for selector in content_selectors:
                        elements = soup.select(selector)
                        for el in elements:
                            desc_text += el.get_text(" ", strip=True) + " "
                    
                    # çµ„åˆæ¨™é¡Œèˆ‡æè¿°ä¾›åˆ†æ
                    full_text_for_analysis = name + " " + desc_text

                    # è¨ˆç®—è¦æ ¼
                    tags = extract_tags(full_text_for_analysis)
                    total_count, unit_price = calculate_unit_price(name, special_price_val, desc_text)

                    print(f"æˆåŠŸæŠ“å–: {name} | ç‰¹åƒ¹: {special_price_val} | æ¨™ç±¤: '{tags}'")

                    all_data.append({
                        "product_name": name,
                        "original_price": original_price_val,
                        "special_price": special_price_val,
                        "total_count": total_count,
                        "unit_price": unit_price,
                        "tags": tags,
                        "image_url": image_url,
                        "product_url": link
                    })
                    
                    success = True # æ¨™è¨˜æˆåŠŸï¼Œè·³å‡º while è¿´åœˆ

                except Exception as e:
                    if "403" in str(e) or "å°é–" in str(e) or "Forbidden" in str(e) or "Access Denied" in str(e):
                        print(f"è¢«å°é–æˆ– 403 éŒ¯èª¤: {e}")
                        retries += 1
                        if retries <= max_retries:
                            print("å•Ÿå‹•å†·å»æ©Ÿåˆ¶ï¼šç­‰å¾… 30 ç§’å¾Œé‡è©¦...")
                            await asyncio.sleep(30)
                            if context:
                                await context.close()
                            context = None # å¼·åˆ¶ä¸‹æ¬¡è¿´åœˆé‡å»º Context
                        else:
                            print(f"æ”¾æ£„æ­¤é€£çµ {link}ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ã€‚")
                    else:
                        print(f"æŠ“å–å¤±æ•— {link}: {e}")
                        break # å…¶ä»–éŒ¯èª¤ä¸é‡è©¦ï¼Œé¿å…ç„¡çª®è¿´åœˆ
        
        await browser.close()

    # å­˜æª”
    if all_data:
        if not os.path.exists('data'):
            os.makedirs('data')
        df = pd.DataFrame(all_data)
        output_path = 'data/d2c_daiken_all_products.csv'
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nå…¨éƒ¨å®Œæˆï¼å…± {len(df)} ç­†è³‡æ–™å·²å„²å­˜è‡³ {output_path}")
    else:
        print("\næœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ã€‚")

if __name__ == '__main__':
    asyncio.run(scrape_daiken_all_products())