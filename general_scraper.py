import requests
import pandas as pd
import time
import re
import random
import os
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup

# ==========================================
# ç”¢å“æ¸…å–®å®šç¾©
# ==========================================
TARGET_KEYWORDS = ["è‘‰é»ƒç´ ", "ç›Šç”ŸèŒ", "é­šæ²¹"]

# User-Agent æ± ï¼šéš¨æ©ŸåŒ–ä»¥é™ä½è¢«å°é–é¢¨éšª
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"
]

# ==========================================
# å·¥å…·å‡½å¼
# ==========================================
# å“ç‰Œç™½åå–®ï¼šå„ªå…ˆåŒ¹é…é€™äº›å“ç‰Œï¼Œé¿å…æŠ“å–éŒ¯èª¤æ¨™é¡Œå‰ç¶´ï¼Œæå‡è³‡æ–™æº–ç¢ºæ€§
BRAND_WHITELIST = [
    "å¤§ç ”ç”Ÿé†«", "ç‡Ÿé¤Šå¸«è¼•é£Ÿ", "Swisse", "Nature's Way", "Blackmores", "GNC",
    "Kemin", "FloraGLO", "Lutemax", "DSM", "BASF", "NOW Foods", "Doctor's Best"
]

def extract_brand(title):
    if not isinstance(title, str): return "æœªæ¨™ç¤º"

    # å„ªå…ˆåŒ¹é…å“ç‰Œç™½åå–®ï¼ˆå¤§å°å¯«ä¸æ•æ„Ÿï¼‰
    for brand in BRAND_WHITELIST:
        if brand.lower() in title.lower():
            return brand

    # å˜—è©¦æŠ“å– ã€ã€‘ æˆ– [] è£¡é¢çš„å“ç‰Œ
    match = re.search(r"[ã€\[](.+?)[ã€‘\]]", title)
    if match:
        return match.group(1).strip()
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä¸”æ¨™é¡Œå¤ é•·ï¼Œæš«æ™‚ç”¨å‰å››å€‹å­—ç•¶å“ç‰Œ
    return title[:4] if len(title) > 4 else "æœªæ¨™ç¤º"

def calculate_unit_price(title, price):
    if not isinstance(title, str): return None, 0
    unit_count = None
    bundle_size = 1

    # æå–ã€Œå–®ç“¶é¡†æ•¸ã€
    match = re.search(r'(\d+)\s*[ç²’é¡†éŒ ]', title)
    if match:
        unit_count = int(match.group(1))

    # æå–ã€Œçµ„æ•¸ã€
    match = re.search(r'(\d+)\s*[å…¥ä»¶ç›’ç½åŒ…]çµ„?', title)
    if match:
        bundle_size = int(match.group(1))
    else:
        match = re.search(r'[xX*]\s*(\d+)', title)
        if match:
            bundle_size = int(match.group(1))

    if unit_count is not None:
        total_count = unit_count * bundle_size
        unit_price = round(price / total_count, 2)
    else:
        total_count = None
        unit_price = 0

    return total_count, unit_price

def extract_tags(text):
    tags = []
    if not isinstance(text, str): return ""

    # 1. å‹æ…‹ (æ¸¸é›¢å‹å„ªæ–¼é…¯åŒ–å‹)
    if re.search(r"æ¸¸é›¢å‹|Free form", text, re.IGNORECASE):
        tags.append("âœ…æ¸¸é›¢å‹")
    elif re.search(r"é…¯åŒ–å‹|Ester", text, re.IGNORECASE):
        tags.append("âš ï¸é…¯åŒ–å‹")

    # 2. åŸæ–™ (FloraGLO ç‚ºå¤§å» æŒ‡æ¨™)
    if re.search(r"FloraGLO|Kemin", text, re.IGNORECASE):
        tags.append("ğŸ’FloraGLO")
    elif re.search(r"Lutemax", text, re.IGNORECASE):
        tags.append("ğŸ’Lutemax")

    # 3. æ¯”ä¾‹ (10:2 é»ƒé‡‘æ¯”ä¾‹)
    if re.search(r"10[:ï¼š]2|10æ¯”2", text):
        tags.append("âš–ï¸10:2æ¯”ä¾‹")

    # 4. è¤‡æ–¹ (è¦ç´…ç´ ã€èŠ±é’ç´ )
    if re.search(r"è¦ç´…ç´ |è—»ç´…ç´ ", text):
        tags.append("ğŸ¦è¦ç´…ç´ ")
    if re.search(r"èŠ±é’ç´ |å±±æ¡‘å­|é»‘é†‹æ —|æ™ºåˆ©é…’æœ", text):
        tags.append("ğŸ«èŠ±é’ç´ ")

    # æ–°å¢ï¼šé€²éšè¤‡æ–¹ (é‡å°æƒ…å¢ƒ)
    if re.search(r"ç»å°¿é…¸|é­šæ²¹|DHA", text):
        tags.append("ğŸ’§æ°´æ½¤é…æ–¹")
    if re.search(r"è¦ç´…ç´ |é»‘è±†", text):
        tags.append("ğŸ¦èˆ’ç·©å°ˆæ³¨")
    if re.search(r"é¦¬å¥‡è“|å±±æ¡‘å­|èŠ±é’ç´ ", text):
        tags.append("ğŸ«å¤œè¦–å®ˆè­·")

    # æ–°å¢ï¼šåŠ‘å‹åµæ¸¬
    if re.search(r"è† å›Š", text):
        tags.append("ğŸ’Šè† å›Š")
    if re.search(r"é£²|å‡", text):
        tags.append("ğŸ§ƒé£²å“/å‡")

    # 5. æª¢é©—èˆ‡èªè­‰ - æ›´æ–°ç‚ºå…·é«”çš„
    if re.search(r"SNQ", text, re.IGNORECASE):
        tags.append("ğŸ…SNQèªè­‰")
    if re.search(r"SGS", text, re.IGNORECASE):
        tags.append("ğŸ›¡ï¸SGSæª¢é©—")
    if re.search(r"åœ‹å®¶èªè­‰", text, re.IGNORECASE):
        tags.append("ğŸ›¡ï¸ç²èªè­‰")

    # å¦‚æœå®Œå…¨æ²’æœ‰æ¨™ç±¤ï¼Œæ¨™è¨˜ç‚ºä¸€èˆ¬
    if not tags:
        return ""

    return " ".join(tags)

# ==========================================
# 1. PChome çˆ¬èŸ² (æ³›åŒ–ç‰ˆ)
# ==========================================
def scrape_pchome(keyword):
    print(f"ğŸš€ [PChome] é–‹å§‹æŠ“å–é—œéµå­—ï¼š{keyword}")
    url = "https://ecshweb.pchome.com.tw/search/v3.3/all/results"
    params = {'q': keyword, 'page': 1, 'sort': 'sale/dc'}
    data_list = []

    try:
        for page in range(1, 4): # æŠ“å‰ 3 é 
            params['page'] = page
            res = requests.get(url, params=params)
            if res.status_code == 200:
                products = res.json().get('prods', [])
                print(f"   ğŸ“„ PChome ç¬¬ {page} é æŠ“åˆ° {len(products)} ç­†...")

                for p in products:
                    # --- é—œéµä¿®æ­£ï¼šåŒæ™‚å˜—è©¦å¤§å°å¯« key ---
                    name = p.get('Name') or p.get('name') or ""
                    # æ¸…æ´—æ¨™é¡Œï¼Œé¿å… CSV éŒ¯ä½
                    name = name.replace(",", " ").replace("\n", " ")

                    # åƒ¹æ ¼æœ‰æ™‚å€™å« Price, price, æˆ–æ˜¯ originPrice
                    price = p.get('Price') or p.get('price') or p.get('originPrice') or 0

                    pid = p.get('Id') or p.get('id')

                    # åœ–ç‰‡ key ä¹Ÿå¯èƒ½è®Š
                    img_filename = p.get('PicS') or p.get('picS') or p.get('PicB') or p.get('picB')
                    # --------------------------------

                    if img_filename:
                        # è£œä¸Š PChome åœ–ç‰‡ç¶²åŸŸ
                        if img_filename.startswith('http'):
                             image_url = img_filename
                        else:
                             image_url = f"https://cs-a.ecimg.tw{img_filename}"
                    else:
                        image_url = "https://dummyimage.com/200x200/cccccc/ffffff.png&text=No+Image"

                    if not pid: continue

                    total_count, unit_price = calculate_unit_price(name, int(price))

                    data_list.append({
                        "source": "PChome",
                        "brand": extract_brand(name),
                        "title": name,
                        "price": int(price),
                        "url": f"https://24h.pchome.com.tw/prod/{pid}",
                        "image_url": image_url,
                        "tags": extract_tags(name),
                        "sales_volume": 0,  # PChome ä¸æä¾›éŠ·é‡æ•¸æ“šï¼Œé è¨­ç‚º 0
                        "raw_data": name,
                        "total_count": total_count,
                        "unit_price": unit_price
                    })
            time.sleep(1)
    except Exception as e:
        print(f"âŒ [PChome] éŒ¯èª¤: {e}")

    return data_list

# ==========================================
# 2. MOMO çˆ¬èŸ² (æ³›åŒ–ç‰ˆ)
# ==========================================
def scrape_momo(keyword, limit=100):
    print(f"ğŸš€ [MOMO] å•Ÿå‹•éš±èº«ç€è¦½å™¨ (éŠ·é‡æ’åº) é—œéµå­—ï¼š{keyword}")
    data_list = []

    with sync_playwright() as p:
        # 1. å•Ÿå‹•åƒæ•¸ï¼šç§»é™¤è‡ªå‹•åŒ–ç‰¹å¾µ
        browser = p.chromium.launch(
            headless=True,
            args=['--disable-blink-features=AutomationControlled', '--no-sandbox']
        )

        # 2. è¨­ç½® User Agentï¼ˆéš¨æ©ŸåŒ–ä»¥é™ä½è¢«å°é–é¢¨éšªï¼‰
        random_user_agent = random.choice(USER_AGENTS)
        context = browser.new_context(
            user_agent=random_user_agent
        )

        # 3. æ³¨å…¥ JS éš±è— webdriver å±¬æ€§
        page = context.new_page()
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        try:
            count = 0
            for page_num in range(1, 4):  # åªçˆ¬å‰ 3 é 
                if count >= limit: break
                print(f"ğŸ”— å‰å¾€ MOMO ç¬¬ {page_num} é ...")
                url = f"https://www.momoshop.com.tw/search/searchShop.jsp?keyword={keyword}&searchType=6&curPage={page_num}"
                page.goto(url)
                time.sleep(random.uniform(2, 5))  # åŠ å…¥éš¨æ©Ÿå»¶é²

                # å¢åŠ è¼‰å…¥ç­‰å¾…æ™‚é–“
                try:
                    page.wait_for_selector(".listGoodsData, .goodsUrl", timeout=8000)
                except:
                    print("â³ MOMO è¼‰å…¥è¼ƒæ…¢ï¼Œç¹¼çºŒå˜—è©¦...")

                # æŠ“å–è³‡æ–™ - èª¿æ•´é¸æ“‡å™¨ç¢ºä¿æŠ“åˆ°æ‰€æœ‰å•†å“
                items = page.locator(".listGoodsData").all()
                if not items: items = page.locator(".goodsUrl").all()
                if not items: items = page.locator("li.goodsItemLi").all()
                if not items: items = page.locator(".EachGood").all()
                if not items: items = page.locator("#CategoryContent li").all()

                print(f"ğŸ“¦ MOMO ç¬¬ {page_num} é æ‰¾åˆ° {len(items)} å€‹å•†å“...")

                for item in items:
                    if count >= limit: break
                    try:
                        title = item.locator(".prdName").first.inner_text()
                        # æ¸…æ´—æ¨™é¡Œä¸­çš„é€—è™Ÿå’Œæ›è¡Œç¬¦ï¼Œé¿å… CSV éŒ¯ä½
                        title = title.replace(",", " ").replace("\n", " ")
                        print(f"   [é€²åº¦] æ­£åœ¨è§£æç¬¬ {count+1}/{limit} ç­†ï¼š{title[:10]}...", end="\r")

                        price_text = item.locator(".price, .money").first.inner_text()
                        price = int(re.sub(r'[^\d]', '', price_text))

                        link = item.get_attribute("href") or item.locator("a").first.get_attribute("href")
                        if link and not link.startswith("http"): link = "https://www.momoshop.com.tw" + link

                        # é€²å…¥å…§é æŠ“å–è©³ç´°è³‡è¨Š - ä½¿ç”¨æ–°åˆ†é é¿å…å½±éŸ¿åˆ—è¡¨é 
                        # å¢åŠ  try-except æ•æ‰ç‰¹å®šçš„è¶…æ™‚éŒ¯èª¤ï¼Œç¢ºä¿æŸä¸€ç­†è³‡æ–™å¤±æ•—ä¸å½±éŸ¿æ•´é«”æŠ“å–
                        inner_text = ""
                        if link:
                            new_page = None
                            try:
                                new_page = context.new_page()
                                new_page.goto(link, wait_until="domcontentloaded", timeout=60000)
                                time.sleep(random.uniform(2, 5))  # åŠ å…¥éš¨æ©Ÿå»¶é²
                                try:
                                    inner_text = new_page.locator('.spec, .description, #spec').first.inner_text()
                                except:
                                    inner_text = ""
                            except (TimeoutError, PlaywrightTimeoutError) as e:
                                print(f"â° å…§é è¶…æ™‚ ({link}): {e} - è·³éæ­¤ç­†ï¼Œç¹¼çºŒä¸‹ä¸€ç­†")
                                inner_text = ""
                            except Exception as e:
                                print(f"âŒ å…§é æŠ“å–å¤±æ•— ({link}): {e}")
                                inner_text = ""
                            finally:
                                if new_page:
                                    try:
                                        new_page.close()
                                    except:
                                        pass

                        # åœ–ç‰‡æŠ“å–
                        image_url = None
                        imgs = item.locator("img").all()
                        for img in imgs:
                            src = img.get_attribute("data-original") or img.get_attribute("src")
                            # éæ¿¾ç„¡æ•ˆåœ–ç‰‡
                            if src and "ecm" not in src and "icon" not in src:
                                if "goodsimg" in src or "i1.momoshop" in src:
                                    image_url = src
                                    break
                                if not image_url and "dummy" not in src and "data:image" not in src:
                                    image_url = src

                        # æ¨™æº–åŒ–åœ–ç‰‡ç¶²å€ï¼šè£œä¸Š "https:"
                        if image_url and image_url.startswith('//'):
                            image_url = 'https:' + image_url
                        
                        if not image_url: image_url = "https://dummyimage.com/200x200/cccccc/ffffff.png&text=MOMO+No+Img"

                        # æŠ“å–éŠ·é‡ - å¦‚æœæŠ“ä¸åˆ°é è¨­ç‚º 0
                        sales_volume = 0
                        try:
                            slogan_text = item.locator(".money .slogan").first.inner_text()
                            match = re.search(r'ç¸½éŠ·é‡\D*(\d+(?:,\d+)*)', slogan_text)  # æ”¾å¯¬ Regex
                            if match:
                                sales_volume = int(match.group(1).replace(',', ''))
                        except:
                            pass

                        # åˆä½µ title å’Œå…§é æ–‡å­—ç”¨æ–¼ extract_tags
                        combined_text = title + " " + inner_text
                        tags = extract_tags(combined_text)

                        total_count, unit_price = calculate_unit_price(title, price)

                        data_list.append({
                            "source": "MOMO",
                            "brand": extract_brand(title),
                            "title": title,
                            "price": price,
                            "url": link,
                            "image_url": image_url,
                            "tags": tags,
                            "sales_volume": sales_volume,
                            "raw_data": title,
                            "total_count": total_count,
                            "unit_price": unit_price
                        })
                        count += 1
                    except Exception as e:
                        print(f"âŒ å•†å“æŠ“å–å¤±æ•—: {e}")
                        # å³ä½¿å¤±æ•—ï¼Œä¹Ÿå˜—è©¦è¨˜éŒ„åŸºæœ¬è³‡æ–™ (æ¨™é¡Œã€åƒ¹æ ¼)
                        try:
                            basic_title = item.locator(".prdName").first.inner_text().replace(",", " ").replace("\n", " ")
                            basic_price_text = item.locator(".price, .money").first.inner_text()
                            basic_price = int(re.sub(r'[^\d]', '', basic_price_text))
                            basic_link = item.get_attribute("href") or item.locator("a").first.get_attribute("href")
                            if basic_link and not basic_link.startswith("http"): basic_link = "https://www.momoshop.com.tw" + basic_link

                            total_count, unit_price = calculate_unit_price(basic_title, basic_price)

                            data_list.append({
                                "source": "MOMO",
                                "brand": extract_brand(basic_title),
                                "title": basic_title,
                                "price": basic_price,
                                "url": basic_link,
                                "image_url": "https://dummyimage.com/200x200/cccccc/ffffff.png&text=MOMO+Basic",
                                "tags": "",
                                "sales_volume": 0,
                                "raw_data": basic_title,
                                "total_count": total_count,
                                "unit_price": unit_price
                            })
                            count += 1
                        except:
                            continue

                time.sleep(1)

        except Exception as e:
            print(f"âŒ [MOMO] éŒ¯èª¤: {e}")
        finally:
            browser.close()

    return data_list

# ==========================================
# ä¸»ç¨‹å¼
# ==========================================
if __name__ == "__main__":
    # å»ºç«‹ data è³‡æ–™å¤¾
    os.makedirs("data", exist_ok=True)

    for keyword in TARGET_KEYWORDS:
        print(f"\nğŸ” é–‹å§‹æŠ“å–é—œéµå­—ï¼š{keyword}")
        # 1. åŸ·è¡Œ PChome
        df_p = pd.DataFrame(scrape_pchome(keyword))

        # 2. åŸ·è¡Œ MOMO (é™åˆ¶å‰ 30 ç­†å•†å“)
        df_m = pd.DataFrame(scrape_momo(keyword, 30))

        # 3. åˆä½µèˆ‡å­˜æª”
        all_df = pd.concat([df_p, df_m], ignore_index=True)

        if not all_df.empty:
            filename = f"data/{keyword}_data.csv"
            all_df.to_csv(filename, index=False, encoding="utf-8-sig")
            print(f"\nâœ… {keyword} è³‡æ–™å­˜æª”å®Œæˆï¼")
            print(f"   PChome: {len(df_p)} ç­†")
            print(f"   MOMO:   {len(df_m)} ç­†")

            # ç°¡å–®æª¢æŸ¥å‰å¹¾ç­† PChome æ˜¯å¦æœ‰æŠ“åˆ°æ¨™é¡Œ
            print(f"\nğŸ” è³‡æ–™æŠ½æ¨£æª¢æŸ¥ ({keyword} PChome):")
            print(df_p[['title', 'price']].head(3))
        else:
            print(f"âš ï¸ {keyword} å®Œå…¨æ²’æŠ“åˆ°è³‡æ–™ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–ç¨‹å¼ç¢¼ã€‚")

        # é—œéµå­—é–“å»¶é²ï¼Œé¿å…å°é›»å•†å¹³å°é€ æˆå¤ªå¤§ç¬é–“æµé‡
        if keyword != TARGET_KEYWORDS[-1]:  # æœ€å¾Œä¸€å€‹ä¸éœ€è¦å»¶é²
            print(f"â³ ä¼‘æ¯ 10 ç§’å¾Œç¹¼çºŒä¸‹ä¸€å€‹é—œéµå­—...")
            time.sleep(10)
