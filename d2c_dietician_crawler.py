import asyncio
import random
import pandas as pd
import os
import re
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
import google.generativeai as genai

# æ³¨æ„ï¼šè«‹ç¢ºä¿ç’°å¢ƒè®Šæ•¸ GOOGLE_API_KEY å·²è¨­å®šï¼Œæˆ–åœ¨æ­¤è™•ç›´æ¥å¡«å…¥æ‚¨çš„ Key
# å¦‚æœæ‚¨å·²åœ¨çµ‚ç«¯æ©Ÿè¨­å®š export GOOGLE_API_KEY="..."ï¼Œé€™è¡Œæœƒè‡ªå‹•è®€å–
# å¦‚æœæ²’æœ‰ï¼Œè«‹å°‡ä¸‹æ–¹çš„ "AIzaSy..." æ›¿æ›ç‚ºæ‚¨çœŸå¯¦çš„ API Key
if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = "AIzaSy..."  # <--- è«‹åœ¨æ­¤è²¼ä¸Šæ‚¨çš„çœŸå¯¦ API Key

async def extract_highlights_with_llm(html_content):
    """
    ä½¿ç”¨ LLM åˆ†æç¶²é å…§å®¹ï¼Œæå–ç”¢å“æ ¸å¿ƒäº®é»ã€‚
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 1. æ¸…æ´— HTMLï¼šç§»é™¤ç„¡é—œçš„æ¨™ç±¤ä»¥æ¸›å°‘ Token ä½¿ç”¨ä¸¦é™ä½é›œè¨Š
    for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'noscript', 'iframe', 'svg', 'button', 'input']):
        tag.decompose()
        
    # 2. å–å¾—ä¸»è¦æ–‡å­—å…§å®¹ (é™åˆ¶é•·åº¦ä»¥å…è¶…é Token ä¸Šé™ï¼Œé€šå¸¸ç”¢å“é‡é»åœ¨å‰ 10000 å­—å…ƒå…§)
    text_content = soup.get_text(separator='\n', strip=True)[:10000] 

    # 3. å®šç¾© LLM Prompt (ä¾æ“šæ‚¨çš„éœ€æ±‚å®¢è£½åŒ–)
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç‡Ÿé¤Šå¸«èˆ‡æ•¸æ“šåˆ†æå¸«ã€‚è«‹åˆ†æä»¥ä¸‹ç”¢å“ç¶²é å…§å®¹ï¼Œä¸¦æå–ç”¢å“çš„ã€æ ¸å¿ƒäº®é»ã€ã€‚
    
    ç¶²é å…§å®¹æ‘˜è¦ï¼š
    {text_content}
    
    ä»»å‹™è¦æ±‚ï¼š
    1. æ‰¾å‡ºè©²ç”¢å“çš„ã€æ ¸å¿ƒäº®é»ã€ã€‚é€™é€šå¸¸åŒ…å«ï¼šå°ˆåˆ©æˆåˆ†ã€åŸæ–™ä¾†æºã€èªè­‰(å¦‚IFOS, SGS)ã€åŠ å·¥å‹æ…‹(å¦‚rTG, æ¸¸é›¢å‹)ã€æˆ–æ˜¯é‡å°ç‰¹å®šæ—ç¾¤çš„è¨­è¨ˆã€‚
    2. è«‹å°‡å…¶æ•´ç†æˆ 3-5 å€‹ç°¡çŸ­çš„é—œéµçŸ­å¥ï¼ˆå¦‚ï¼šã€æ¡ç”¨ FloraGLOÂ® æ¸¸é›¢å‹è‘‰é»ƒç´ ã€ã€ã€84% é«˜æ¿ƒåº¦ rTG é­šæ²¹ã€ï¼‰ã€‚
    3. è‡ªå‹•ç§»é™¤æè¿°ä¸­çš„å»£å‘Šè©ï¼ˆå¦‚ã€Œè¶…å€¼ç‰¹æƒ ã€ã€ã€Œæ‰‹åˆ€å¿«æ¶ã€ã€ã€Œé™æ™‚ä¸‹æ®ºã€ï¼‰ï¼Œåªä¿ç•™æŠ€è¡“è¦æ ¼èˆ‡ç”¢å“å„ªå‹¢ã€‚
    4. è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼ŒåŒ…å« 'product_name' (ç”¢å“åç¨±) å’Œ 'product_highlights' (ä»¥åˆ†è™Ÿåˆ†éš”çš„äº®é»å­—ä¸²) å…©å€‹æ¬„ä½ã€‚
    
    è¼¸å‡ºç¯„ä¾‹ï¼š
    {{
        "product_name": "è¦–æ˜“é©è‘‰é»ƒç´ ",
        "product_highlights": "æ¸¸é›¢å‹è‘‰é»ƒç´ 15mg;æ·»åŠ è¦ç´…ç´ èˆ‡æ™ºåˆ©é…’æœ;FloraGLOÂ®å°ˆåˆ©åŸæ–™;å…¨ç´ å¯é£Ÿ"
    }}
    """

    try:
        # æª¢æŸ¥ API Key
        if "GOOGLE_API_KEY" not in os.environ:
            print("âš ï¸ æœªè¨­å®š GOOGLE_API_KEYï¼Œè·³é AI åˆ†æ")
            return {"product_name": "Unknown", "product_highlights": ""}

        # è¨­å®š Gemini (ä½¿ç”¨ gemini-1.5-flash æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ä¸”æ”¯æ´ JSON mode)
        genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
        model = genai.GenerativeModel('gemini-1.5-flash', generation_config={"response_mime_type": "application/json", "temperature": 0.2})
        
        # å‘¼å« API
        full_prompt = f"You are a helpful assistant that extracts structured product data from HTML text.\n\n{prompt}"
        response = await model.generate_content_async(full_prompt)

        # ç›£æ§ Token ä½¿ç”¨é‡
        if response.usage_metadata:
            print(f"   ğŸ“Š Token ä½¿ç”¨é‡: è¼¸å…¥ {response.usage_metadata.prompt_token_count} + è¼¸å‡º {response.usage_metadata.candidates_token_count} = ç¸½è¨ˆ {response.usage_metadata.total_token_count}")
        
        result = json.loads(response.text)
        return result
    except Exception as e:
        print(f"LLM åˆ†æå¤±æ•—: {e}")
        # å›å‚³é è¨­ç©ºå€¼ä»¥å…ç¨‹å¼å´©æ½°
        return {"product_name": "Unknown", "product_highlights": ""}

async def random_sleep(min_sec=3, max_sec=7):
    """ç•°æ­¥ç­‰å¾…ä¸€å€‹éš¨æ©Ÿçš„ç§’æ•¸ï¼Œæ¨¡æ“¬çœŸäººåœé “ã€‚"""
    sleep_time = random.uniform(min_sec, max_sec)
    # print(f"Simulating human behavior: waiting for {sleep_time:.2f} seconds...")
    await asyncio.sleep(sleep_time)

def calculate_unit_price(title, price, description=""):
    """å¾æ¨™é¡Œè¨ˆç®—ç¸½é¡†ç²’æ•¸èˆ‡å–®ä½åƒ¹æ ¼ (é‡å°ç‡Ÿé¤Šå¸«è¼•é£Ÿå„ªåŒ–)"""
    if not isinstance(title, str): return None, 0
    unit_count, bundle_size = None, 1
    
    # 1. å°‹æ‰¾å–®å“æ•¸é‡ (å„ªå…ˆç´šï¼šæè¿°ä¸­çš„æ˜ç¢ºå®šç¾© > æ¨™é¡Œ > æè¿°ä¸­çš„æ¨æ¸¬)
    
    # ç­–ç•¥ A: æè¿°ä¸­çš„æ˜ç¢ºå®šç¾© (ä¾‹å¦‚ "æ¯ç›’ 60 é¡†", "å…§å®¹é‡ï¼š30æ¢")
    if description:
        spec_match = re.search(r'(?:æ¯ç›’|æ¯ç“¶|å…§å®¹é‡|è¦æ ¼|å®¹é‡)[ï¼š:\s]*(\d+)\s*[ç²’é¡†éŒ åŒ…æ¢å…¥]', description)
        if spec_match:
            unit_count = int(spec_match.group(1))

    # ç­–ç•¥ B: æ¨™é¡Œä¸­çš„æ•¸é‡
    if not unit_count:
        count_regex = r'(\d+)\s*[ç²’é¡†éŒ åŒ…æ¢å…¥]'
        match = re.search(count_regex, title)
        if match: 
            unit_count = int(match.group(1))

    # ç­–ç•¥ C: æè¿°ä¸­çš„æ¨æ¸¬ (æ‰¾æœ€å¤§çš„æ•¸å­—ï¼Œé€šå¸¸ç¸½æ•¸ > æ¯æ—¥é£Ÿç”¨é‡)
    if not unit_count and description:
        # æ‰¾å‡ºæ‰€æœ‰ "æ•¸å­— + å–®ä½" çš„çµ„åˆ
        matches = re.findall(r'(\d+)\s*[ç²’é¡†éŒ åŒ…æ¢å…¥]', description)
        if matches:
            # éæ¿¾æ‰å°æ–¼ 10 çš„æ•¸å­— (å‡è¨­å–®å“æ•¸é‡é€šå¸¸ >= 10ï¼Œé¿é–‹ "æ¯æ—¥2é¡†" é€™ç¨®è³‡è¨Š)
            candidates = [int(m) for m in matches if int(m) >= 10]
            if candidates:
                unit_count = max(candidates) # å–æœ€å¤§å€¼æœ€ä¿éšª

    # 2. å°‹æ‰¾çµ„æ•¸ (Bundle Size)
    # åŒ¹é… x3, *3, 3å…¥, 3ç›’çµ„
    bundle_match = re.search(r'[xX*]\s*(\d{1,2})\b', title)
    if bundle_match:
        bundle_size = int(bundle_match.group(1))
    else:
        # åŒ¹é… "3å…¥", "3ä»¶çµ„", "3ç›’çµ„"
        bundle_match = re.search(r'[\s\uff0c\(\uff08](\d{1,2})\s*[å…¥ä»¶çµ„ç›’]', title)
        if bundle_match: bundle_size = int(bundle_match.group(1))
    
    # é˜²å‘†ï¼šå¦‚æœçµ„æ•¸å¤§æ–¼ 10 ä¸”èˆ‡å–®å“æ•¸é‡ç›¸åŒï¼Œæ¥µå¯èƒ½æ˜¯èª¤åˆ¤
    if unit_count and bundle_size > 10 and unit_count == bundle_size:
        bundle_size = 1
        
    if unit_count:
        total_count = unit_count * bundle_size
        u_price = round(price / total_count, 2) if price else 0
        return total_count, u_price
    return None, 0

def extract_tags(text):
    """å¾æ–‡æœ¬ä¸­æå–ç”¢å“æ¨™ç±¤"""
    tags = []
    if not isinstance(text, str): return ""
    
    # è‘‰é»ƒç´ /è­·çœ¼
    if re.search(r"æ¸¸é›¢å‹|Free form", text, re.IGNORECASE): tags.append("âœ…æ¸¸é›¢å‹")
    if re.search(r"FloraGLO", text, re.IGNORECASE): tags.append("ğŸ’FloraGLO")
    if re.search(r"10[:ï¼š]2", text): tags.append("âš–ï¸10:2æ¯”ä¾‹")
    
    # é­šæ²¹
    if re.search(r"Omega-?3", text, re.IGNORECASE): tags.append("ğŸŸOmega-3")
    if re.search(r"rTG", text, re.IGNORECASE): tags.append("ğŸ§¬rTGå‹")
    if re.search(r"IFOS", text, re.IGNORECASE): tags.append("ğŸ†IFOSèªè­‰")
    if re.search(r"80%|84%|90%", text): tags.append("ğŸ“ˆé«˜æ¿ƒåº¦")

    # ç›Šç”ŸèŒ/é…µç´ 
    if re.search(r"ç›Šç”ŸèŒ|ä¹³é…¸èŒ", text): tags.append("ğŸ¦ ç›Šç”ŸèŒ")
    if re.search(r"300å„„|260å„„|1000å„„", text): tags.append("ğŸ”¢é«˜èŒæ•¸")
    if re.search(r"ä¿è­‰èŒæ•¸", text): tags.append("ğŸ›¡ï¸ä¿è­‰èŒæ•¸")
    if re.search(r"ç„¡æ·»åŠ ", text): tags.append("ğŸŒ¿ç„¡æ·»åŠ ")

    # èªè­‰
    if re.search(r"SNQ", text, re.IGNORECASE): tags.append("ğŸ…SNQèªè­‰")
    if re.search(r"SGS", text, re.IGNORECASE): tags.append("ğŸ›¡ï¸SGSæª¢é©—")
    if re.search(r"A\.A\. Clean Label", text, re.IGNORECASE): tags.append("ğŸŒ±æ½”æ·¨æ¨™ç« ")
    
    return " ".join(tags) if tags else ""

async def scrape_dietician_all_products():
    """
    æ‰¹é‡æŠ“å–ç‡Ÿé¤Šå¸«è¼•é£Ÿæ‰€æœ‰ç”¢å“è³‡æ–™ã€‚
    """
    list_url = "https://www.dietician.com.tw/"
    base_url = "https://www.dietician.com.tw"
    all_data = []
    
    headless_mode = True 

    async with async_playwright() as p:
        print(f"å•Ÿå‹•ç€è¦½å™¨ (Headless: {headless_mode})...")
        browser = await p.chromium.launch(headless=headless_mode)
        
        # --- æ­¥é©Ÿ 1: å–å¾—æ‰€æœ‰ç”¢å“é€£çµ ---
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        await stealth_async(page)

        print(f"æ­£åœ¨å‰å¾€é¦–é æŠ“å–é¸å–®é€£çµ: {list_url}")
        await page.goto(list_url, wait_until='networkidle', timeout=60000)
        await random_sleep(2, 3)

        # æ»¾å‹•é é¢ç¢ºä¿è¼‰å…¥
        print("æ­£åœ¨æ»¾å‹•é é¢ä»¥è¼‰å…¥åˆ—è¡¨...")
        for _ in range(3):
            await page.evaluate('window.scrollBy(0, window.innerHeight)')
            await asyncio.sleep(1)

        # è§£æé€£çµ
        content = await page.content()
        soup = BeautifulSoup(content, 'html.parser')
        
        product_links = set()
        current_url = page.url

        for a in soup.find_all('a', href=True):
            href = a['href']
            full_link = urljoin(current_url, href)
            
            # ç¯©é¸æ¢ä»¶ï¼šåŒ…å« /products/item/ (æ ¹æ“šä½¿ç”¨è€…æä¾›çš„ç¯„ä¾‹é€£çµçµæ§‹)
            if '/products/item/' in full_link and base_url in full_link:
                product_links.add(full_link)
        
        links = list(product_links)
        print(f"å…±ç™¼ç¾ {len(links)} å€‹ä¸é‡è¤‡çš„ç”¢å“é€£çµã€‚")
        
        await context.close()
        context = None
        page = None

        # --- æ­¥é©Ÿ 2: æ‰¹é‡æŠ“å–è©³æƒ… ---
        for i, link in enumerate(links):
            retries = 0
            max_retries = 2
            success = False

            while retries <= max_retries and not success:
                # æ¯ 30 ç­†è«‹æ±‚é é˜²æ€§é‡ç½® Context
                if context is None or (i > 0 and i % 30 == 0 and retries == 0):
                    if context:
                        print(f"\n--- å·²è™•ç† {i} ç­†è³‡æ–™ï¼Œå•Ÿå‹•é é˜²æ€§å†·å»èˆ‡ç’°å¢ƒé‡ç½® ---")
                        print("å†·å» 20 ç§’...")
                        await asyncio.sleep(20)
                        await context.close()
                    
                    print("æ­£åœ¨å»ºç«‹æ–°çš„ç€è¦½å™¨ç’°å¢ƒï¼ˆæ›´æ›èº«ä»½ï¼‰...")
                    context = await browser.new_context(
                        viewport={'width': 1920, 'height': 1080},
                        user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
                    )
                    page = await context.new_page()
                    await stealth_async(page)

                if retries > 0:
                    print(f"\n[{i+1}/{len(links)}] æ­£åœ¨é‡è©¦: {link} (ç¬¬ {retries} æ¬¡é‡è©¦)")
                else:
                    print(f"\n[{i+1}/{len(links)}] æ­£åœ¨è™•ç†: {link}")
            
                try:
                    await page.goto(link, wait_until='networkidle', timeout=60000)
                    await random_sleep(3, 7) # éš¨æ©Ÿç­‰å¾… 3-7 ç§’

                    # ç­‰å¾…åƒ¹æ ¼ç›¸é—œæ–‡å­—å‡ºç¾ï¼Œç¢ºä¿å‹•æ…‹å…§å®¹å·²è¼‰å…¥
                    try:
                        await page.locator('body').filter(has_text="NT$").first.wait_for(timeout=5000)
                    except:
                        pass

                    # ç­‰å¾…é—œéµå…ƒç´  (åƒ¹æ ¼æˆ–æ¨™é¡Œ)
                    try:
                        await page.locator('h1').first.wait_for(state='visible', timeout=10000)
                    except:
                        print("ç­‰å¾…æ¨™é¡Œè¶…æ™‚ï¼Œå˜—è©¦ç›´æ¥è§£æ...")

                    content = await page.content()
                    soup = BeautifulSoup(content, 'html.parser')

                    # 1. ç”¢å“åç¨±
                    h1 = soup.find('h1')
                    name = h1.get_text(strip=True) if h1 else ""

                    # æª¢æŸ¥æ˜¯å¦è¢«å°é–
                    if "403" in name or "Forbidden" in name:
                        raise Exception(f"åµæ¸¬åˆ°å°é–é é¢ (Title: {name})")

                    # 2. åƒ¹æ ¼è§£æ (å¤šé‡ç­–ç•¥)
                    price_text_all = soup.get_text()
                    prices = []
                    original_price_val = 0
                    special_price_val = 0
                    
                    # ç­–ç•¥ A: JSON-LD (æœ€æº–ç¢ºï¼Œç‡Ÿé¤Šå¸«è¼•é£Ÿæœ‰ä½¿ç”¨)
                    json_ld = soup.find('script', type='application/ld+json')
                    if json_ld:
                        try:
                            data = json.loads(json_ld.string)
                            # è™•ç†å¯èƒ½çš„åˆ—è¡¨æˆ–å–®ä¸€ç‰©ä»¶
                            if isinstance(data, list): data = data[0]
                            
                            if data.get('@type') == 'Product':
                                if not name: name = data.get('name', "Unknown")
                                offers = data.get('offers', {})
                                if isinstance(offers, dict):
                                    p = offers.get('price')
                                    if p:
                                        special_price_val = int(float(p))
                                        original_price_val = special_price_val # æš«æ™‚è¨­ç‚ºç›¸åŒ
                        except:
                            pass

                    # ç­–ç•¥ B: Meta Tags (å‚™ç”¨)
                    meta_price = soup.find("meta", property="product:price:amount") or \
                                 soup.find("meta", property="og:price:amount")
                    if not special_price_val and meta_price and meta_price.get("content"):
                        try:
                            special_price_val = int(float(meta_price["content"]))
                            original_price_val = special_price_val
                        except:
                            pass

                    # ç­–ç•¥ C: å…§æ–‡æ­£å‰‡æœç´¢ (æœ€å¾Œæ‰‹æ®µ)
                    if not special_price_val:
                        matches = re.findall(r'(?:NT\$?|\$)\s*(\d{1,3}(?:,\d{3})*|\d+)', price_text_all, re.IGNORECASE)
                        for m in matches:
                            try:
                                prices.append(int(m.replace(',', '')))
                            except:
                                pass
                        if prices:
                            prices = sorted(list(set(prices)))
                            if len(prices) > 1:
                                original_price_val = prices[-1]
                                special_price_val = prices[0]
                            else:
                                original_price_val = prices[0]
                                special_price_val = prices[0]
                    
                    if not name: name = "Unknown"

                    # 3. åœ–ç‰‡
                    image_url = ""
                    og_img = soup.find("meta", property="og:image")
                    if og_img and og_img.get("content"):
                        image_url = urljoin(base_url, og_img["content"])
                    
                    # 4. è¦æ ¼èˆ‡æ¨™ç±¤
                    # æŠ“å–ä¸»è¦å…§å®¹å€å¡Š
                    desc_text = ""
                    # å„ªå…ˆæŠ“å– .description (ç‡Ÿé¤Šå¸«è¼•é£Ÿçš„è¦æ ¼é€šå¸¸åœ¨é€™è£¡)
                    for selector in [".description", ".product-detail", ".content", "main"]:
                        elements = soup.select(selector)
                        for el in elements:
                            desc_text += el.get_text(" ", strip=True) + " "
                    
                    full_text_for_analysis = name + " " + desc_text
                    tags = extract_tags(full_text_for_analysis)
                    total_count, unit_price = calculate_unit_price(name, special_price_val, desc_text)
                    
                    # 5. AI äº®é»åˆ†æ
                    print("   ğŸ¤– æ­£åœ¨å‘¼å« AI é€²è¡Œèªç¾©åˆ†æ...")
                    ai_result = await extract_highlights_with_llm(content)
                    highlights = ai_result.get("product_highlights", "")

                    print(f"æˆåŠŸæŠ“å–: {name} | ç‰¹åƒ¹: {special_price_val} | è¦æ ¼: {total_count} | æ¨™ç±¤: '{tags}' | äº®é»: {highlights[:20]}...")

                    all_data.append({
                        "product_name": name,
                        "original_price": original_price_val,
                        "special_price": special_price_val,
                        "total_count": total_count,
                        "unit_price": unit_price,
                        "tags": tags,
                        "product_highlights": highlights,
                        "image_url": image_url,
                        "product_url": link
                    })
                    
                    success = True

                except Exception as e:
                    if "403" in str(e) or "Forbidden" in str(e):
                        print(f"è¢«å°é–æˆ– 403 éŒ¯èª¤: {e}")
                        retries += 1
                        if retries <= max_retries:
                            print("å•Ÿå‹•å†·å»æ©Ÿåˆ¶ï¼šç­‰å¾… 30 ç§’å¾Œé‡è©¦...")
                            await asyncio.sleep(30)
                            if context: await context.close()
                            context = None
                        else:
                            print(f"æ”¾æ£„æ­¤é€£çµ {link}")
                    else:
                        print(f"æŠ“å–å¤±æ•— {link}: {e}")
                        break
        
        await browser.close()

    # å­˜æª”
    if all_data:
        if not os.path.exists('data'):
            os.makedirs('data')
        df = pd.DataFrame(all_data)
        output_path = 'data/d2c_dietician_products.csv'
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nå…¨éƒ¨å®Œæˆï¼å…± {len(df)} ç­†è³‡æ–™å·²å„²å­˜è‡³ {output_path}")
    else:
        print("\næœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ã€‚")

if __name__ == '__main__':
    asyncio.run(scrape_dietician_all_products())